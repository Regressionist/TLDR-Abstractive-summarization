{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "from batchify import Corpus\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from model import Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=Corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,corpus):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    encoded, hidden=encoder(input_tensor)\n",
    "    batch_size=target_tensor.size(0)\n",
    "    decoder_input = np.zeros((batch_size,1))\n",
    "    decoder_input[:]=corpus.w2i['<sos>']\n",
    "    decoder_input=torch.from_numpy(decoder_input).cuda().detach()\n",
    "    dec_len=target_tensor.size(1)\n",
    "    decoded, hidden, outputs=decoder(encoded,hidden,decoder_input.long(),dec_len,target_tensor)\n",
    "    s=outputs.size(1)\n",
    "    loss=0\n",
    "    for i in range(s):\n",
    "        loss+=criterion(outputs[:,i,:],target_tensor[:,i])\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss/target_tensor.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(input_tensor, target_tensor, encoder, decoder, criterion,corpus):\n",
    "    with torch.no_grad():\n",
    "        encoded, hidden=encoder(input_tensor.long())\n",
    "        batch_size=target_tensor.size(0)\n",
    "        decoder_input = np.zeros((batch_size,1))\n",
    "        decoder_input[:]=corpus.w2i['<sos>']\n",
    "        decoder_input=torch.from_numpy(decoder_input).cuda().detach()\n",
    "        dec_len=target_tensor.size(1)\n",
    "        decoded, hidden,outputs=decoder(encoded,hidden,decoder_input.long(),dec_len,target_tensor,val=True)\n",
    "        s=outputs.size(1)\n",
    "        loss=0\n",
    "        for i in range(s):\n",
    "            loss+=criterion(outputs[:,i,:],target_tensor[:,i].long())\n",
    "        \n",
    "    return loss/target_tensor.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=Encoder(vocab_size=50000, embedding_size=250, hidden_size=250)\n",
    "decoder=Decoder(vocab_size=50000, embedding_dim=250, hidden_dim=250)\n",
    "encoder=encoder.to(device)\n",
    "decoder=decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.Adagrad(encoder.parameters(), lr=0.15, initial_accumulator_value=0.1)\n",
    "decoder_optimizer = optim.Adagrad(decoder.parameters(), lr=0.15, initial_accumulator_value=0.1)\n",
    "criterion=nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (attn): Linear(in_features=500, out_features=400, bias=True)\n",
       "  (attn_combine): Linear(in_features=500, out_features=250, bias=True)\n",
       "  (linear1): Linear(in_features=500, out_features=250, bias=True)\n",
       "  (linear2): Linear(in_features=500, out_features=250, bias=True)\n",
       "  (embedding): Embedding(50000, 250)\n",
       "  (gru): GRU(250, 250, batch_first=True)\n",
       "  (linear): Linear(in_features=250, out_features=50000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('model.pth')\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "step = checkpoint['step']\n",
    "tl = checkpoint['training_loss']\n",
    "vl = checkpoint['validation_loss']\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15160 9960\n"
     ]
    }
   ],
   "source": [
    "corpus.counter=15160\n",
    "print(corpus.counter,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 9961/20000 | Training Loss: 5.336607933044434 | Validation Loss: 6.302183628082275\n",
      "Step: 9962/20000 | Training Loss: 5.715422630310059 | Validation Loss: 6.294032573699951\n",
      "Step: 9963/20000 | Training Loss: 5.420490264892578 | Validation Loss: 6.303720951080322\n",
      "Step: 9964/20000 | Training Loss: 5.215981960296631 | Validation Loss: 6.309159278869629\n",
      "Step: 9965/20000 | Training Loss: 5.363572120666504 | Validation Loss: 6.310832500457764\n",
      "Step: 9966/20000 | Training Loss: 5.6544036865234375 | Validation Loss: 6.300436973571777\n",
      "Step: 9967/20000 | Training Loss: 5.662583351135254 | Validation Loss: 6.289553642272949\n",
      "Step: 9968/20000 | Training Loss: 5.521991729736328 | Validation Loss: 6.2934184074401855\n",
      "Step: 9969/20000 | Training Loss: 6.0064473152160645 | Validation Loss: 6.303558826446533\n",
      "Step: 9970/20000 | Training Loss: 5.026932239532471 | Validation Loss: 6.308955669403076\n",
      "Step: 9971/20000 | Training Loss: 5.810401916503906 | Validation Loss: 6.303990364074707\n",
      "Step: 9972/20000 | Training Loss: 5.058270454406738 | Validation Loss: 6.318553924560547\n",
      "Step: 9973/20000 | Training Loss: 5.486616134643555 | Validation Loss: 6.315951824188232\n",
      "Step: 9974/20000 | Training Loss: 5.428045272827148 | Validation Loss: 6.305213928222656\n",
      "Step: 9975/20000 | Training Loss: 5.2342095375061035 | Validation Loss: 6.309718132019043\n",
      "Step: 9976/20000 | Training Loss: 5.136669158935547 | Validation Loss: 6.306254863739014\n",
      "Step: 9977/20000 | Training Loss: 4.799567222595215 | Validation Loss: 6.332372188568115\n",
      "Step: 9978/20000 | Training Loss: 5.648448944091797 | Validation Loss: 6.3003644943237305\n",
      "Step: 9979/20000 | Training Loss: 5.569325923919678 | Validation Loss: 6.2964606285095215\n",
      "Step: 9980/20000 | Training Loss: 5.354928970336914 | Validation Loss: 6.298532962799072\n",
      "Step: 9981/20000 | Training Loss: 5.239527225494385 | Validation Loss: 6.29953670501709\n",
      "Step: 9982/20000 | Training Loss: 6.163586616516113 | Validation Loss: 6.288479328155518\n",
      "Step: 9983/20000 | Training Loss: 4.843395233154297 | Validation Loss: 6.315882682800293\n",
      "Step: 9984/20000 | Training Loss: 5.3780741691589355 | Validation Loss: 6.308570861816406\n",
      "Step: 9985/20000 | Training Loss: 5.9577789306640625 | Validation Loss: 6.298550128936768\n",
      "Step: 9986/20000 | Training Loss: 4.966911315917969 | Validation Loss: 6.3322343826293945\n",
      "Step: 9987/20000 | Training Loss: 5.510091781616211 | Validation Loss: 6.311830043792725\n",
      "Step: 9988/20000 | Training Loss: 5.218294620513916 | Validation Loss: 6.31644344329834\n",
      "Step: 9989/20000 | Training Loss: 5.425806522369385 | Validation Loss: 6.300278186798096\n",
      "Step: 9990/20000 | Training Loss: 5.3919525146484375 | Validation Loss: 6.2938103675842285\n",
      "Step: 9991/20000 | Training Loss: 5.2173357009887695 | Validation Loss: 6.301705837249756\n",
      "Step: 9992/20000 | Training Loss: 5.617666244506836 | Validation Loss: 6.298842906951904\n",
      "Step: 9993/20000 | Training Loss: 5.398280143737793 | Validation Loss: 6.307254791259766\n",
      "Step: 9994/20000 | Training Loss: 5.989733695983887 | Validation Loss: 6.2897047996521\n",
      "Step: 9995/20000 | Training Loss: 5.657652854919434 | Validation Loss: 6.287802219390869\n",
      "Step: 9996/20000 | Training Loss: 5.428826332092285 | Validation Loss: 6.292333602905273\n",
      "Step: 9997/20000 | Training Loss: 5.159631729125977 | Validation Loss: 6.31569766998291\n",
      "Step: 9998/20000 | Training Loss: 5.191728115081787 | Validation Loss: 6.378904342651367\n",
      "Step: 9999/20000 | Training Loss: 5.276243686676025 | Validation Loss: 6.348028182983398\n",
      "Step: 10000/20000 | Training Loss: 6.010956287384033 | Validation Loss: 6.31007194519043\n",
      "Step: 10001/20000 | Training Loss: 4.975022792816162 | Validation Loss: 6.338010311126709\n",
      "Step: 10002/20000 | Training Loss: 5.509394645690918 | Validation Loss: 6.30926513671875\n",
      "Step: 10003/20000 | Training Loss: 5.530334949493408 | Validation Loss: 6.311808109283447\n",
      "Step: 10004/20000 | Training Loss: 5.641356468200684 | Validation Loss: 6.31043004989624\n",
      "Step: 10005/20000 | Training Loss: 5.355438709259033 | Validation Loss: 6.31195068359375\n",
      "Step: 10006/20000 | Training Loss: 4.834073543548584 | Validation Loss: 6.329877853393555\n",
      "Step: 10007/20000 | Training Loss: 5.35730504989624 | Validation Loss: 6.3242034912109375\n",
      "Step: 10008/20000 | Training Loss: 5.308230876922607 | Validation Loss: 6.320339202880859\n",
      "Step: 10009/20000 | Training Loss: 5.468559265136719 | Validation Loss: 6.315530300140381\n",
      "Step: 10010/20000 | Training Loss: 5.514266014099121 | Validation Loss: 6.307010650634766\n",
      "Step: 10011/20000 | Training Loss: 6.1702117919921875 | Validation Loss: 6.308023452758789\n",
      "Step: 10012/20000 | Training Loss: 5.432804107666016 | Validation Loss: 6.310560703277588\n",
      "Step: 10013/20000 | Training Loss: 5.3438286781311035 | Validation Loss: 6.316894054412842\n",
      "Step: 10014/20000 | Training Loss: 5.43312406539917 | Validation Loss: 6.308335304260254\n",
      "Step: 10015/20000 | Training Loss: 5.448172569274902 | Validation Loss: 6.325122833251953\n",
      "Step: 10016/20000 | Training Loss: 5.560210704803467 | Validation Loss: 6.316501617431641\n",
      "Step: 10017/20000 | Training Loss: 5.39607048034668 | Validation Loss: 6.314972877502441\n",
      "Step: 10018/20000 | Training Loss: 4.390344619750977 | Validation Loss: 6.367781162261963\n",
      "Step: 10019/20000 | Training Loss: 5.529725074768066 | Validation Loss: 6.3352789878845215\n",
      "Step: 10020/20000 | Training Loss: 5.472898483276367 | Validation Loss: 6.317049980163574\n",
      "Step: 10021/20000 | Training Loss: 5.33575439453125 | Validation Loss: 6.316479682922363\n",
      "Step: 10022/20000 | Training Loss: 5.20898962020874 | Validation Loss: 6.313791751861572\n",
      "Step: 10023/20000 | Training Loss: 5.451068878173828 | Validation Loss: 6.307973384857178\n",
      "Step: 10024/20000 | Training Loss: 5.303075313568115 | Validation Loss: 6.348276138305664\n",
      "Step: 10025/20000 | Training Loss: 5.062327861785889 | Validation Loss: 6.344829082489014\n",
      "Step: 10026/20000 | Training Loss: 5.789377212524414 | Validation Loss: 6.315714359283447\n",
      "Step: 10027/20000 | Training Loss: 5.120345115661621 | Validation Loss: 6.311248302459717\n",
      "Step: 10028/20000 | Training Loss: 5.378543376922607 | Validation Loss: 6.313638210296631\n",
      "Step: 10029/20000 | Training Loss: 5.578719139099121 | Validation Loss: 6.317093372344971\n",
      "Step: 10030/20000 | Training Loss: 5.6469879150390625 | Validation Loss: 6.315557479858398\n",
      "Step: 10031/20000 | Training Loss: 5.539551258087158 | Validation Loss: 6.318523406982422\n",
      "Step: 10032/20000 | Training Loss: 5.534115314483643 | Validation Loss: 6.309712886810303\n",
      "Step: 10033/20000 | Training Loss: 5.40180778503418 | Validation Loss: 6.314147472381592\n",
      "Step: 10034/20000 | Training Loss: 5.719822406768799 | Validation Loss: 6.310417175292969\n",
      "Step: 10035/20000 | Training Loss: 5.813794136047363 | Validation Loss: 6.3063764572143555\n",
      "Step: 10036/20000 | Training Loss: 6.004817008972168 | Validation Loss: 6.301172733306885\n",
      "Step: 10037/20000 | Training Loss: 4.966386318206787 | Validation Loss: 6.315971374511719\n",
      "Step: 10038/20000 | Training Loss: 5.735857009887695 | Validation Loss: 6.304092884063721\n",
      "Step: 10039/20000 | Training Loss: 6.007170677185059 | Validation Loss: 6.301455497741699\n",
      "Step: 10040/20000 | Training Loss: 5.793691635131836 | Validation Loss: 6.307101726531982\n",
      "Step: 10041/20000 | Training Loss: 5.2872748374938965 | Validation Loss: 6.304712295532227\n",
      "Step: 10042/20000 | Training Loss: 5.3602094650268555 | Validation Loss: 6.310751914978027\n",
      "Step: 10043/20000 | Training Loss: 5.127510070800781 | Validation Loss: 6.316151142120361\n",
      "Step: 10044/20000 | Training Loss: 5.198659896850586 | Validation Loss: 6.314672470092773\n",
      "Step: 10045/20000 | Training Loss: 5.218266010284424 | Validation Loss: 6.316887378692627\n",
      "Step: 10046/20000 | Training Loss: 5.665315628051758 | Validation Loss: 6.297579288482666\n",
      "Step: 10047/20000 | Training Loss: 5.983854293823242 | Validation Loss: 6.2883100509643555\n",
      "Step: 10048/20000 | Training Loss: 5.654038429260254 | Validation Loss: 6.284176349639893\n",
      "%---Saving the model---%\n",
      "Step: 10049/20000 | Training Loss: 5.463195323944092 | Validation Loss: 6.287863731384277\n",
      "Step: 10050/20000 | Training Loss: 4.871811866760254 | Validation Loss: 6.3124003410339355\n",
      "Step: 10051/20000 | Training Loss: 4.875032901763916 | Validation Loss: 6.323231220245361\n",
      "Step: 10052/20000 | Training Loss: 5.239089012145996 | Validation Loss: 6.3136773109436035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10053/20000 | Training Loss: 5.185098171234131 | Validation Loss: 6.311402797698975\n",
      "Step: 10054/20000 | Training Loss: 6.142661094665527 | Validation Loss: 6.2993998527526855\n",
      "Step: 10055/20000 | Training Loss: 5.228925704956055 | Validation Loss: 6.305155277252197\n",
      "Step: 10056/20000 | Training Loss: 5.202666759490967 | Validation Loss: 6.3082075119018555\n",
      "Step: 10057/20000 | Training Loss: 6.134332656860352 | Validation Loss: 6.30678653717041\n",
      "Step: 10058/20000 | Training Loss: 5.232161998748779 | Validation Loss: 6.302610874176025\n",
      "Step: 10059/20000 | Training Loss: 5.092964172363281 | Validation Loss: 6.314047813415527\n",
      "Step: 10060/20000 | Training Loss: 5.749735355377197 | Validation Loss: 6.296676158905029\n",
      "Step: 10061/20000 | Training Loss: 5.19612455368042 | Validation Loss: 6.3007988929748535\n",
      "Step: 10062/20000 | Training Loss: 5.6711626052856445 | Validation Loss: 6.3036112785339355\n",
      "Step: 10063/20000 | Training Loss: 5.430266380310059 | Validation Loss: 6.302990436553955\n",
      "Step: 10064/20000 | Training Loss: 6.013168811798096 | Validation Loss: 6.283565998077393\n",
      "%---Saving the model---%\n",
      "Step: 10065/20000 | Training Loss: 4.670322895050049 | Validation Loss: 6.326854228973389\n",
      "Step: 10066/20000 | Training Loss: 5.350056171417236 | Validation Loss: 6.3137030601501465\n",
      "Step: 10067/20000 | Training Loss: 5.29150390625 | Validation Loss: 6.307676315307617\n",
      "Step: 10068/20000 | Training Loss: 5.936559677124023 | Validation Loss: 6.302994251251221\n",
      "Step: 10069/20000 | Training Loss: 5.270511150360107 | Validation Loss: 6.309017658233643\n",
      "Step: 10070/20000 | Training Loss: 5.448391914367676 | Validation Loss: 6.310609340667725\n",
      "Step: 10071/20000 | Training Loss: 6.31903076171875 | Validation Loss: 6.3016157150268555\n",
      "Step: 10072/20000 | Training Loss: 5.8980913162231445 | Validation Loss: 6.294149398803711\n",
      "Step: 10073/20000 | Training Loss: 5.64225435256958 | Validation Loss: 6.288512706756592\n",
      "Step: 10074/20000 | Training Loss: 5.015198707580566 | Validation Loss: 6.309803485870361\n",
      "Step: 10075/20000 | Training Loss: 5.528364658355713 | Validation Loss: 6.30571985244751\n",
      "Step: 10076/20000 | Training Loss: 5.780933380126953 | Validation Loss: 6.292502403259277\n",
      "Step: 10077/20000 | Training Loss: 5.4520649909973145 | Validation Loss: 6.30940580368042\n",
      "Step: 10078/20000 | Training Loss: 5.705538749694824 | Validation Loss: 6.295194149017334\n",
      "Step: 10079/20000 | Training Loss: 5.413877964019775 | Validation Loss: 6.298058032989502\n",
      "Step: 10080/20000 | Training Loss: 6.084801197052002 | Validation Loss: 6.288471221923828\n",
      "Step: 10081/20000 | Training Loss: 5.269461154937744 | Validation Loss: 6.295633792877197\n",
      "Step: 10082/20000 | Training Loss: 5.953079700469971 | Validation Loss: 6.301506519317627\n",
      "Step: 10083/20000 | Training Loss: 5.347891330718994 | Validation Loss: 6.303843021392822\n",
      "Step: 10084/20000 | Training Loss: 5.922487258911133 | Validation Loss: 6.289175510406494\n",
      "Step: 10085/20000 | Training Loss: 5.794606685638428 | Validation Loss: 6.301276206970215\n",
      "Step: 10086/20000 | Training Loss: 5.4247307777404785 | Validation Loss: 6.2988481521606445\n",
      "Step: 10087/20000 | Training Loss: 5.244418621063232 | Validation Loss: 6.321479797363281\n",
      "Step: 10088/20000 | Training Loss: 5.1059112548828125 | Validation Loss: 6.3288187980651855\n",
      "Step: 10089/20000 | Training Loss: 5.689733982086182 | Validation Loss: 6.302994728088379\n",
      "Step: 10090/20000 | Training Loss: 5.312016010284424 | Validation Loss: 6.312025547027588\n",
      "Step: 10091/20000 | Training Loss: 5.7327375411987305 | Validation Loss: 6.309247970581055\n",
      "Step: 10092/20000 | Training Loss: 6.046133518218994 | Validation Loss: 6.288962364196777\n",
      "Step: 10093/20000 | Training Loss: 5.93620491027832 | Validation Loss: 6.29266881942749\n",
      "Step: 10094/20000 | Training Loss: 5.590790748596191 | Validation Loss: 6.294303894042969\n",
      "Step: 10095/20000 | Training Loss: 5.974503040313721 | Validation Loss: 6.2963361740112305\n",
      "Step: 10096/20000 | Training Loss: 5.794463634490967 | Validation Loss: 6.318749904632568\n",
      "Step: 10097/20000 | Training Loss: 5.634188652038574 | Validation Loss: 6.298768520355225\n",
      "Step: 10098/20000 | Training Loss: 5.102335453033447 | Validation Loss: 6.319131851196289\n",
      "Step: 10099/20000 | Training Loss: 5.403144359588623 | Validation Loss: 6.3149871826171875\n",
      "Step: 10100/20000 | Training Loss: 5.138988018035889 | Validation Loss: 6.332474708557129\n",
      "Step: 10101/20000 | Training Loss: 5.596339702606201 | Validation Loss: 6.318596839904785\n",
      "Step: 10102/20000 | Training Loss: 5.6146697998046875 | Validation Loss: 6.315192222595215\n",
      "Step: 10103/20000 | Training Loss: 5.839478492736816 | Validation Loss: 6.315884590148926\n",
      "Step: 10104/20000 | Training Loss: 4.908591270446777 | Validation Loss: 6.3265061378479\n",
      "Step: 10105/20000 | Training Loss: 5.685882568359375 | Validation Loss: 6.339845657348633\n",
      "Step: 10106/20000 | Training Loss: 5.398562431335449 | Validation Loss: 6.351970672607422\n",
      "Step: 10107/20000 | Training Loss: 5.5456223487854 | Validation Loss: 6.329105854034424\n",
      "Step: 10108/20000 | Training Loss: 5.9149580001831055 | Validation Loss: 6.3318705558776855\n",
      "Step: 10109/20000 | Training Loss: 5.410593032836914 | Validation Loss: 6.325587272644043\n",
      "Step: 10110/20000 | Training Loss: 5.642571449279785 | Validation Loss: 6.354894161224365\n",
      "Step: 10111/20000 | Training Loss: 5.362176895141602 | Validation Loss: 6.3182525634765625\n",
      "Step: 10112/20000 | Training Loss: 5.066954612731934 | Validation Loss: 6.382021427154541\n",
      "Step: 10113/20000 | Training Loss: 5.92371129989624 | Validation Loss: 6.434345722198486\n",
      "Step: 10114/20000 | Training Loss: 6.1803998947143555 | Validation Loss: 6.388094425201416\n",
      "Step: 10115/20000 | Training Loss: 6.035995006561279 | Validation Loss: 6.33879280090332\n",
      "Step: 10116/20000 | Training Loss: 5.088577747344971 | Validation Loss: 6.336361885070801\n",
      "Step: 10117/20000 | Training Loss: 6.39129638671875 | Validation Loss: 6.293478012084961\n",
      "Step: 10118/20000 | Training Loss: 5.269698143005371 | Validation Loss: 6.31008243560791\n",
      "Step: 10119/20000 | Training Loss: 5.601811408996582 | Validation Loss: 6.311392307281494\n",
      "Step: 10120/20000 | Training Loss: 5.796414852142334 | Validation Loss: 6.306512832641602\n",
      "Step: 10121/20000 | Training Loss: 5.877448081970215 | Validation Loss: 6.305692195892334\n",
      "Step: 10122/20000 | Training Loss: 5.357150554656982 | Validation Loss: 6.297616481781006\n",
      "Step: 10123/20000 | Training Loss: 5.725731372833252 | Validation Loss: 6.293798446655273\n",
      "Step: 10124/20000 | Training Loss: 5.233516693115234 | Validation Loss: 6.304510593414307\n",
      "Step: 10125/20000 | Training Loss: 5.611530303955078 | Validation Loss: 6.325661659240723\n",
      "Step: 10126/20000 | Training Loss: 5.3825249671936035 | Validation Loss: 6.300597667694092\n",
      "Step: 10127/20000 | Training Loss: 5.36234188079834 | Validation Loss: 6.299466609954834\n",
      "Step: 10128/20000 | Training Loss: 5.783761501312256 | Validation Loss: 6.292757987976074\n",
      "Step: 10129/20000 | Training Loss: 5.673783302307129 | Validation Loss: 6.284068584442139\n",
      "Step: 10130/20000 | Training Loss: 5.821680545806885 | Validation Loss: 6.2874579429626465\n",
      "Step: 10131/20000 | Training Loss: 5.022029876708984 | Validation Loss: 6.302545547485352\n",
      "Step: 10132/20000 | Training Loss: 5.700621604919434 | Validation Loss: 6.286648750305176\n",
      "Step: 10133/20000 | Training Loss: 5.8614959716796875 | Validation Loss: 6.289906024932861\n",
      "Step: 10134/20000 | Training Loss: 5.50661563873291 | Validation Loss: 6.297248840332031\n",
      "Step: 10135/20000 | Training Loss: 5.640471935272217 | Validation Loss: 6.293572902679443\n",
      "Step: 10136/20000 | Training Loss: 5.272204875946045 | Validation Loss: 6.294614315032959\n",
      "Step: 10137/20000 | Training Loss: 5.895776271820068 | Validation Loss: 6.2879486083984375\n",
      "Step: 10138/20000 | Training Loss: 5.467437267303467 | Validation Loss: 6.341594696044922\n",
      "Step: 10139/20000 | Training Loss: 5.595735549926758 | Validation Loss: 6.2895636558532715\n",
      "Step: 10140/20000 | Training Loss: 5.684617519378662 | Validation Loss: 6.290978908538818\n",
      "Step: 10141/20000 | Training Loss: 5.580674648284912 | Validation Loss: 6.291335582733154\n",
      "Step: 10142/20000 | Training Loss: 5.070254325866699 | Validation Loss: 6.301141262054443\n",
      "Step: 10143/20000 | Training Loss: 5.644003868103027 | Validation Loss: 6.294464588165283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10144/20000 | Training Loss: 5.589088439941406 | Validation Loss: 6.287568092346191\n",
      "Step: 10145/20000 | Training Loss: 4.6944966316223145 | Validation Loss: 6.34027624130249\n",
      "Step: 10146/20000 | Training Loss: 5.238375663757324 | Validation Loss: 6.329432964324951\n",
      "Step: 10147/20000 | Training Loss: 5.853811264038086 | Validation Loss: 6.303055286407471\n",
      "Step: 10148/20000 | Training Loss: 4.667355060577393 | Validation Loss: 6.333863735198975\n",
      "Step: 10149/20000 | Training Loss: 5.32357120513916 | Validation Loss: 6.31191349029541\n",
      "Step: 10150/20000 | Training Loss: 5.708406925201416 | Validation Loss: 6.303318023681641\n",
      "Step: 10151/20000 | Training Loss: 5.6416192054748535 | Validation Loss: 6.302514553070068\n",
      "Step: 10152/20000 | Training Loss: 5.76275634765625 | Validation Loss: 6.289306163787842\n",
      "Step: 10153/20000 | Training Loss: 5.2579450607299805 | Validation Loss: 6.303128719329834\n",
      "Step: 10154/20000 | Training Loss: 6.187253952026367 | Validation Loss: 6.282688140869141\n",
      "%---Saving the model---%\n",
      "Step: 10155/20000 | Training Loss: 5.5129265785217285 | Validation Loss: 6.29487943649292\n",
      "Step: 10156/20000 | Training Loss: 5.618356227874756 | Validation Loss: 6.294210910797119\n",
      "Step: 10157/20000 | Training Loss: 5.554090976715088 | Validation Loss: 6.303798675537109\n",
      "Step: 10158/20000 | Training Loss: 5.4860029220581055 | Validation Loss: 6.297694206237793\n",
      "Step: 10159/20000 | Training Loss: 4.97252893447876 | Validation Loss: 6.321021556854248\n",
      "Step: 10160/20000 | Training Loss: 5.839917182922363 | Validation Loss: 6.291220188140869\n",
      "Step: 10161/20000 | Training Loss: 6.235892295837402 | Validation Loss: 6.2756242752075195\n",
      "%---Saving the model---%\n",
      "Step: 10162/20000 | Training Loss: 5.201083183288574 | Validation Loss: 6.285769462585449\n",
      "Step: 10163/20000 | Training Loss: 5.652144908905029 | Validation Loss: 6.277719497680664\n",
      "Step: 10164/20000 | Training Loss: 5.52502965927124 | Validation Loss: 6.285381317138672\n",
      "Step: 10165/20000 | Training Loss: 5.314783573150635 | Validation Loss: 6.30229377746582\n",
      "Step: 10166/20000 | Training Loss: 4.472109317779541 | Validation Loss: 6.36574125289917\n",
      "Step: 10167/20000 | Training Loss: 5.2043776512146 | Validation Loss: 6.349330425262451\n",
      "Step: 10168/20000 | Training Loss: 5.475730895996094 | Validation Loss: 6.327722549438477\n",
      "Step: 10169/20000 | Training Loss: 5.65640926361084 | Validation Loss: 6.313457012176514\n",
      "Step: 10170/20000 | Training Loss: 6.039772033691406 | Validation Loss: 6.2967634201049805\n",
      "Step: 10171/20000 | Training Loss: 5.744527339935303 | Validation Loss: 6.299921035766602\n",
      "Step: 10172/20000 | Training Loss: 5.420055389404297 | Validation Loss: 6.294598579406738\n",
      "Step: 10173/20000 | Training Loss: 4.534144401550293 | Validation Loss: 6.3236188888549805\n",
      "Step: 10174/20000 | Training Loss: 5.366024494171143 | Validation Loss: 6.305426597595215\n",
      "Step: 10175/20000 | Training Loss: 5.639290809631348 | Validation Loss: 6.287787914276123\n",
      "Step: 10176/20000 | Training Loss: 5.4229817390441895 | Validation Loss: 6.291908264160156\n",
      "Step: 10177/20000 | Training Loss: 5.8392744064331055 | Validation Loss: 6.29007625579834\n",
      "Step: 10178/20000 | Training Loss: 5.427397727966309 | Validation Loss: 6.289348602294922\n",
      "Step: 10179/20000 | Training Loss: 5.503523826599121 | Validation Loss: 6.307036876678467\n",
      "Step: 10180/20000 | Training Loss: 5.19615364074707 | Validation Loss: 6.294515132904053\n",
      "Step: 10181/20000 | Training Loss: 5.64509916305542 | Validation Loss: 6.3010663986206055\n",
      "Step: 10182/20000 | Training Loss: 5.467569351196289 | Validation Loss: 6.286160469055176\n",
      "Step: 10183/20000 | Training Loss: 5.532497406005859 | Validation Loss: 6.28611946105957\n",
      "Step: 10184/20000 | Training Loss: 5.737274646759033 | Validation Loss: 6.282810211181641\n",
      "Step: 10185/20000 | Training Loss: 6.239465713500977 | Validation Loss: 6.2835798263549805\n",
      "Step: 10186/20000 | Training Loss: 5.366311073303223 | Validation Loss: 6.295960903167725\n",
      "Step: 10187/20000 | Training Loss: 5.446255683898926 | Validation Loss: 6.2911834716796875\n",
      "Step: 10188/20000 | Training Loss: 5.169421672821045 | Validation Loss: 6.296712398529053\n",
      "Step: 10189/20000 | Training Loss: 5.335792541503906 | Validation Loss: 6.302043437957764\n",
      "Step: 10190/20000 | Training Loss: 5.8148322105407715 | Validation Loss: 6.292471885681152\n",
      "Step: 10191/20000 | Training Loss: 5.545976161956787 | Validation Loss: 6.29263162612915\n",
      "Step: 10192/20000 | Training Loss: 5.9054765701293945 | Validation Loss: 6.2848711013793945\n",
      "Step: 10193/20000 | Training Loss: 5.67334508895874 | Validation Loss: 6.282188892364502\n",
      "Step: 10194/20000 | Training Loss: 5.06796407699585 | Validation Loss: 6.304173946380615\n",
      "Step: 10195/20000 | Training Loss: 5.144713401794434 | Validation Loss: 6.312785625457764\n",
      "Step: 10196/20000 | Training Loss: 5.4053802490234375 | Validation Loss: 6.304912090301514\n",
      "Step: 10197/20000 | Training Loss: 5.191580295562744 | Validation Loss: 6.311224937438965\n",
      "Step: 10198/20000 | Training Loss: 5.495152950286865 | Validation Loss: 6.300151348114014\n",
      "Step: 10199/20000 | Training Loss: 5.237698078155518 | Validation Loss: 6.301517486572266\n",
      "Step: 10200/20000 | Training Loss: 5.492829322814941 | Validation Loss: 6.292370796203613\n",
      "Step: 10201/20000 | Training Loss: 5.572281360626221 | Validation Loss: 6.293642997741699\n",
      "Step: 10202/20000 | Training Loss: 5.728228569030762 | Validation Loss: 6.2888264656066895\n",
      "Step: 10203/20000 | Training Loss: 5.533437728881836 | Validation Loss: 6.290832042694092\n",
      "Step: 10204/20000 | Training Loss: 5.7242584228515625 | Validation Loss: 6.285487174987793\n",
      "Step: 10205/20000 | Training Loss: 4.431727886199951 | Validation Loss: 6.3564558029174805\n",
      "Step: 10206/20000 | Training Loss: 5.89488410949707 | Validation Loss: 6.297495365142822\n",
      "Step: 10207/20000 | Training Loss: 5.521556854248047 | Validation Loss: 6.302938938140869\n",
      "Step: 10208/20000 | Training Loss: 5.533044815063477 | Validation Loss: 6.300241470336914\n",
      "Step: 10209/20000 | Training Loss: 4.601413249969482 | Validation Loss: 6.344605445861816\n",
      "Step: 10210/20000 | Training Loss: 5.736617088317871 | Validation Loss: 6.302779197692871\n",
      "Step: 10211/20000 | Training Loss: 6.0543084144592285 | Validation Loss: 6.294024467468262\n",
      "Step: 10212/20000 | Training Loss: 5.09617805480957 | Validation Loss: 6.310354232788086\n",
      "Step: 10213/20000 | Training Loss: 6.085569858551025 | Validation Loss: 6.293808460235596\n",
      "Step: 10214/20000 | Training Loss: 5.776858329772949 | Validation Loss: 6.291032791137695\n",
      "Step: 10215/20000 | Training Loss: 5.050289630889893 | Validation Loss: 6.301283359527588\n",
      "Step: 10216/20000 | Training Loss: 4.8559184074401855 | Validation Loss: 6.338624000549316\n",
      "Step: 10217/20000 | Training Loss: 4.963050842285156 | Validation Loss: 6.330174922943115\n",
      "Step: 10218/20000 | Training Loss: 5.3255839347839355 | Validation Loss: 6.318500995635986\n",
      "Step: 10219/20000 | Training Loss: 5.197861671447754 | Validation Loss: 6.3176960945129395\n",
      "Step: 10220/20000 | Training Loss: 5.202561378479004 | Validation Loss: 6.312119483947754\n",
      "Step: 10221/20000 | Training Loss: 5.71771240234375 | Validation Loss: 6.293194770812988\n",
      "Step: 10222/20000 | Training Loss: 5.25101375579834 | Validation Loss: 6.290535926818848\n",
      "Step: 10223/20000 | Training Loss: 5.368928909301758 | Validation Loss: 6.2957763671875\n",
      "Step: 10224/20000 | Training Loss: 6.233893871307373 | Validation Loss: 6.286858558654785\n",
      "Step: 10225/20000 | Training Loss: 4.364450931549072 | Validation Loss: 6.360204696655273\n",
      "Step: 10226/20000 | Training Loss: 4.807962894439697 | Validation Loss: 6.335984706878662\n",
      "Step: 10227/20000 | Training Loss: 5.665742874145508 | Validation Loss: 6.343815803527832\n",
      "Step: 10228/20000 | Training Loss: 5.420210838317871 | Validation Loss: 6.303182601928711\n",
      "Step: 10229/20000 | Training Loss: 5.367555618286133 | Validation Loss: 6.292527198791504\n",
      "Step: 10230/20000 | Training Loss: 5.549761772155762 | Validation Loss: 6.289501667022705\n",
      "Step: 10231/20000 | Training Loss: 5.443783283233643 | Validation Loss: 6.290796279907227\n",
      "Step: 10232/20000 | Training Loss: 5.7703776359558105 | Validation Loss: 6.2851176261901855\n",
      "Step: 10233/20000 | Training Loss: 5.758198261260986 | Validation Loss: 6.282530784606934\n",
      "Step: 10234/20000 | Training Loss: 6.148583889007568 | Validation Loss: 6.286155700683594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10235/20000 | Training Loss: 5.428486347198486 | Validation Loss: 6.287701606750488\n",
      "Step: 10236/20000 | Training Loss: 6.367542266845703 | Validation Loss: 6.304799556732178\n",
      "Step: 10237/20000 | Training Loss: 5.354739665985107 | Validation Loss: 6.300777435302734\n",
      "Step: 10238/20000 | Training Loss: 5.828014373779297 | Validation Loss: 6.299764633178711\n",
      "Step: 10239/20000 | Training Loss: 5.468513488769531 | Validation Loss: 6.300422668457031\n",
      "Step: 10240/20000 | Training Loss: 5.623467922210693 | Validation Loss: 6.310715675354004\n",
      "Step: 10241/20000 | Training Loss: 4.968943119049072 | Validation Loss: 6.311724662780762\n",
      "Step: 10242/20000 | Training Loss: 6.022042274475098 | Validation Loss: 6.289191246032715\n",
      "Step: 10243/20000 | Training Loss: 5.6338701248168945 | Validation Loss: 6.291934967041016\n",
      "Step: 10244/20000 | Training Loss: 5.807989120483398 | Validation Loss: 6.304866790771484\n",
      "Step: 10245/20000 | Training Loss: 5.232584476470947 | Validation Loss: 6.316832065582275\n",
      "Step: 10246/20000 | Training Loss: 5.222622394561768 | Validation Loss: 6.332625389099121\n",
      "Step: 10247/20000 | Training Loss: 6.089340686798096 | Validation Loss: 6.2872209548950195\n",
      "Step: 10248/20000 | Training Loss: 5.588895797729492 | Validation Loss: 6.360523223876953\n",
      "Step: 10249/20000 | Training Loss: 5.443662643432617 | Validation Loss: 6.2950053215026855\n",
      "Step: 10250/20000 | Training Loss: 6.046951770782471 | Validation Loss: 6.287469863891602\n",
      "Step: 10251/20000 | Training Loss: 5.120441436767578 | Validation Loss: 6.303681373596191\n",
      "Step: 10252/20000 | Training Loss: 5.662189960479736 | Validation Loss: 6.289474964141846\n",
      "Step: 10253/20000 | Training Loss: 5.210988998413086 | Validation Loss: 6.2967000007629395\n",
      "Step: 10254/20000 | Training Loss: 5.58512544631958 | Validation Loss: 6.293125152587891\n",
      "Step: 10255/20000 | Training Loss: 6.280184745788574 | Validation Loss: 6.2787322998046875\n",
      "Step: 10256/20000 | Training Loss: 5.602799415588379 | Validation Loss: 6.288093090057373\n",
      "Step: 10257/20000 | Training Loss: 5.666282653808594 | Validation Loss: 6.280653476715088\n",
      "Step: 10258/20000 | Training Loss: 5.242290019989014 | Validation Loss: 6.292203903198242\n",
      "Step: 10259/20000 | Training Loss: 5.81026554107666 | Validation Loss: 6.296048164367676\n",
      "Step: 10260/20000 | Training Loss: 5.649008750915527 | Validation Loss: 6.295594692230225\n",
      "Step: 10261/20000 | Training Loss: 5.802122592926025 | Validation Loss: 6.306764602661133\n",
      "Step: 10262/20000 | Training Loss: 5.787900924682617 | Validation Loss: 6.290483474731445\n",
      "Step: 10263/20000 | Training Loss: 5.256598472595215 | Validation Loss: 6.303329944610596\n",
      "Step: 10264/20000 | Training Loss: 5.698617458343506 | Validation Loss: 6.290925025939941\n",
      "Step: 10265/20000 | Training Loss: 5.169499397277832 | Validation Loss: 6.311347961425781\n",
      "Step: 10266/20000 | Training Loss: 5.413488864898682 | Validation Loss: 6.307405948638916\n",
      "Step: 10267/20000 | Training Loss: 5.20644474029541 | Validation Loss: 6.316544055938721\n",
      "Step: 10268/20000 | Training Loss: 5.994400501251221 | Validation Loss: 6.322073936462402\n",
      "Step: 10269/20000 | Training Loss: 5.670520305633545 | Validation Loss: 6.301014423370361\n",
      "Step: 10270/20000 | Training Loss: 5.782804012298584 | Validation Loss: 6.298640251159668\n",
      "Step: 10271/20000 | Training Loss: 5.124959945678711 | Validation Loss: 6.304727077484131\n",
      "Step: 10272/20000 | Training Loss: 5.747200965881348 | Validation Loss: 6.321649551391602\n",
      "Step: 10273/20000 | Training Loss: 6.273828506469727 | Validation Loss: 6.307919979095459\n",
      "Step: 10274/20000 | Training Loss: 5.930386543273926 | Validation Loss: 6.308287143707275\n",
      "Step: 10275/20000 | Training Loss: 6.044022560119629 | Validation Loss: 6.297567367553711\n",
      "Step: 10276/20000 | Training Loss: 6.220211029052734 | Validation Loss: 6.297593593597412\n",
      "Step: 10277/20000 | Training Loss: 5.199487209320068 | Validation Loss: 6.310948371887207\n",
      "Step: 10278/20000 | Training Loss: 5.9189043045043945 | Validation Loss: 6.292257308959961\n",
      "Step: 10279/20000 | Training Loss: 4.865024566650391 | Validation Loss: 6.315246105194092\n",
      "Step: 10280/20000 | Training Loss: 5.458345890045166 | Validation Loss: 6.305657386779785\n",
      "Step: 10281/20000 | Training Loss: 5.551548004150391 | Validation Loss: 6.289436340332031\n",
      "Step: 10282/20000 | Training Loss: 5.60385799407959 | Validation Loss: 6.3172078132629395\n",
      "Step: 10283/20000 | Training Loss: 5.305883407592773 | Validation Loss: 6.295517921447754\n",
      "Step: 10284/20000 | Training Loss: 6.292318820953369 | Validation Loss: 6.272421836853027\n",
      "%---Saving the model---%\n",
      "Step: 10285/20000 | Training Loss: 5.866984844207764 | Validation Loss: 6.27285623550415\n",
      "Step: 10286/20000 | Training Loss: 5.466970443725586 | Validation Loss: 6.278713703155518\n",
      "Step: 10287/20000 | Training Loss: 5.45086669921875 | Validation Loss: 6.270033836364746\n",
      "%---Saving the model---%\n",
      "Step: 10288/20000 | Training Loss: 5.25149393081665 | Validation Loss: 6.2862348556518555\n",
      "Step: 10289/20000 | Training Loss: 5.289704322814941 | Validation Loss: 6.2758660316467285\n",
      "Step: 10290/20000 | Training Loss: 6.15670919418335 | Validation Loss: 6.255316257476807\n",
      "%---Saving the model---%\n",
      "Step: 10291/20000 | Training Loss: 4.863779067993164 | Validation Loss: 6.272486209869385\n",
      "Step: 10292/20000 | Training Loss: 6.5175065994262695 | Validation Loss: 6.262467861175537\n",
      "Step: 10293/20000 | Training Loss: 5.666081428527832 | Validation Loss: 6.2662224769592285\n",
      "Step: 10294/20000 | Training Loss: 5.75419282913208 | Validation Loss: 6.268893718719482\n",
      "Step: 10295/20000 | Training Loss: 5.86063814163208 | Validation Loss: 6.273933410644531\n",
      "Step: 10296/20000 | Training Loss: 5.744678974151611 | Validation Loss: 6.267594337463379\n",
      "Step: 10297/20000 | Training Loss: 5.879417896270752 | Validation Loss: 6.277790546417236\n",
      "Step: 10298/20000 | Training Loss: 6.117290496826172 | Validation Loss: 6.27339506149292\n",
      "Step: 10299/20000 | Training Loss: 5.958670139312744 | Validation Loss: 6.281006336212158\n",
      "Step: 10300/20000 | Training Loss: 5.898987293243408 | Validation Loss: 6.277054786682129\n",
      "Step: 10301/20000 | Training Loss: 5.643638610839844 | Validation Loss: 6.275414943695068\n",
      "Step: 10302/20000 | Training Loss: 5.091673851013184 | Validation Loss: 6.283908367156982\n",
      "Step: 10303/20000 | Training Loss: 5.281988620758057 | Validation Loss: 6.286255359649658\n",
      "Step: 10304/20000 | Training Loss: 5.716353416442871 | Validation Loss: 6.2852983474731445\n",
      "Step: 10305/20000 | Training Loss: 5.305329322814941 | Validation Loss: 6.295170307159424\n",
      "Step: 10306/20000 | Training Loss: 5.293470859527588 | Validation Loss: 6.296143531799316\n",
      "Step: 10307/20000 | Training Loss: 5.697127819061279 | Validation Loss: 6.277803421020508\n",
      "Step: 10308/20000 | Training Loss: 5.880765914916992 | Validation Loss: 6.282194137573242\n",
      "Step: 10309/20000 | Training Loss: 5.59027624130249 | Validation Loss: 6.275563716888428\n",
      "Step: 10310/20000 | Training Loss: 5.859329700469971 | Validation Loss: 6.279937267303467\n",
      "Step: 10311/20000 | Training Loss: 5.591719627380371 | Validation Loss: 6.273404598236084\n",
      "Step: 10312/20000 | Training Loss: 5.997451305389404 | Validation Loss: 6.270552635192871\n",
      "Step: 10313/20000 | Training Loss: 5.423114776611328 | Validation Loss: 6.280017375946045\n",
      "Step: 10314/20000 | Training Loss: 5.3662028312683105 | Validation Loss: 6.287534236907959\n",
      "Step: 10315/20000 | Training Loss: 5.378879547119141 | Validation Loss: 6.288493633270264\n",
      "Step: 10316/20000 | Training Loss: 5.088414669036865 | Validation Loss: 6.300915718078613\n",
      "Step: 10317/20000 | Training Loss: 5.578245162963867 | Validation Loss: 6.295647621154785\n",
      "Step: 10318/20000 | Training Loss: 5.491161346435547 | Validation Loss: 6.298593044281006\n",
      "Step: 10319/20000 | Training Loss: 5.493359565734863 | Validation Loss: 6.292728900909424\n",
      "Step: 10320/20000 | Training Loss: 5.133566856384277 | Validation Loss: 6.303868293762207\n",
      "Step: 10321/20000 | Training Loss: 5.551426410675049 | Validation Loss: 6.295038223266602\n",
      "Step: 10322/20000 | Training Loss: 5.318017959594727 | Validation Loss: 6.298912525177002\n",
      "Step: 10323/20000 | Training Loss: 5.189198970794678 | Validation Loss: 6.3124284744262695\n",
      "Step: 10324/20000 | Training Loss: 5.1541643142700195 | Validation Loss: 6.3224358558654785\n",
      "Step: 10325/20000 | Training Loss: 6.091385364532471 | Validation Loss: 6.290549278259277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10326/20000 | Training Loss: 5.19545841217041 | Validation Loss: 6.295201778411865\n",
      "Step: 10327/20000 | Training Loss: 5.8711419105529785 | Validation Loss: 6.3362016677856445\n",
      "Step: 10328/20000 | Training Loss: 5.810329914093018 | Validation Loss: 6.281959056854248\n",
      "Step: 10329/20000 | Training Loss: 6.29758358001709 | Validation Loss: 6.276095390319824\n",
      "Step: 10330/20000 | Training Loss: 5.833550930023193 | Validation Loss: 6.277763366699219\n",
      "Step: 10331/20000 | Training Loss: 5.660676956176758 | Validation Loss: 6.30195426940918\n",
      "Step: 10332/20000 | Training Loss: 6.303869247436523 | Validation Loss: 6.2795915603637695\n",
      "Step: 10333/20000 | Training Loss: 5.467462062835693 | Validation Loss: 6.2984724044799805\n",
      "Step: 10334/20000 | Training Loss: 6.099415302276611 | Validation Loss: 6.280705451965332\n",
      "Step: 10335/20000 | Training Loss: 5.561647891998291 | Validation Loss: 6.2809929847717285\n",
      "Step: 10336/20000 | Training Loss: 5.969916820526123 | Validation Loss: 6.282216548919678\n",
      "Step: 10337/20000 | Training Loss: 5.681312561035156 | Validation Loss: 6.291453838348389\n",
      "Step: 10338/20000 | Training Loss: 5.577871799468994 | Validation Loss: 6.287031650543213\n",
      "Step: 10339/20000 | Training Loss: 5.988285541534424 | Validation Loss: 6.283815860748291\n",
      "Step: 10340/20000 | Training Loss: 5.937478542327881 | Validation Loss: 6.280789375305176\n",
      "Step: 10341/20000 | Training Loss: 5.974231719970703 | Validation Loss: 6.291534423828125\n",
      "Step: 10342/20000 | Training Loss: 5.359157562255859 | Validation Loss: 6.288150310516357\n",
      "Step: 10343/20000 | Training Loss: 5.061984539031982 | Validation Loss: 6.3053059577941895\n",
      "Step: 10344/20000 | Training Loss: 5.699209690093994 | Validation Loss: 6.3025078773498535\n",
      "Step: 10345/20000 | Training Loss: 5.012718200683594 | Validation Loss: 6.310970306396484\n",
      "Step: 10346/20000 | Training Loss: 5.7833662033081055 | Validation Loss: 6.29423189163208\n",
      "Step: 10347/20000 | Training Loss: 6.406296253204346 | Validation Loss: 6.289586544036865\n",
      "Step: 10348/20000 | Training Loss: 5.707577228546143 | Validation Loss: 6.3128275871276855\n",
      "Step: 10349/20000 | Training Loss: 5.150476455688477 | Validation Loss: 6.321059226989746\n",
      "Step: 10350/20000 | Training Loss: 5.448040008544922 | Validation Loss: 6.326068878173828\n",
      "Step: 10351/20000 | Training Loss: 6.030627727508545 | Validation Loss: 6.3056640625\n",
      "Step: 10352/20000 | Training Loss: 5.772718906402588 | Validation Loss: 6.326930999755859\n",
      "Step: 10353/20000 | Training Loss: 6.187645435333252 | Validation Loss: 6.3100433349609375\n",
      "Step: 10354/20000 | Training Loss: 5.422166347503662 | Validation Loss: 6.306578636169434\n",
      "Step: 10355/20000 | Training Loss: 5.21916389465332 | Validation Loss: 6.3142194747924805\n",
      "Step: 10356/20000 | Training Loss: 5.858720302581787 | Validation Loss: 6.310205459594727\n",
      "Step: 10357/20000 | Training Loss: 5.997256755828857 | Validation Loss: 6.299246311187744\n",
      "Step: 10358/20000 | Training Loss: 6.057541847229004 | Validation Loss: 6.305027961730957\n",
      "Step: 10359/20000 | Training Loss: 5.259556770324707 | Validation Loss: 6.34993314743042\n",
      "Step: 10360/20000 | Training Loss: 5.611296653747559 | Validation Loss: 6.298196792602539\n",
      "Step: 10361/20000 | Training Loss: 5.483678340911865 | Validation Loss: 6.325509071350098\n",
      "Step: 10362/20000 | Training Loss: 6.008458137512207 | Validation Loss: 6.293851375579834\n",
      "Step: 10363/20000 | Training Loss: 5.494208335876465 | Validation Loss: 6.285833835601807\n",
      "Step: 10364/20000 | Training Loss: 5.44730806350708 | Validation Loss: 6.291412353515625\n",
      "Step: 10365/20000 | Training Loss: 5.722279071807861 | Validation Loss: 6.277800559997559\n",
      "Step: 10366/20000 | Training Loss: 5.759159564971924 | Validation Loss: 6.2722578048706055\n",
      "Step: 10367/20000 | Training Loss: 5.379002571105957 | Validation Loss: 6.2786664962768555\n",
      "Step: 10368/20000 | Training Loss: 5.619427680969238 | Validation Loss: 6.278037071228027\n",
      "Step: 10369/20000 | Training Loss: 5.24827241897583 | Validation Loss: 6.298520565032959\n",
      "Step: 10370/20000 | Training Loss: 4.8504743576049805 | Validation Loss: 6.313826084136963\n",
      "Step: 10371/20000 | Training Loss: 5.990117073059082 | Validation Loss: 6.2874040603637695\n",
      "Step: 10372/20000 | Training Loss: 5.658894062042236 | Validation Loss: 6.281282424926758\n",
      "Step: 10373/20000 | Training Loss: 5.559175491333008 | Validation Loss: 6.286646842956543\n",
      "Step: 10374/20000 | Training Loss: 5.840776443481445 | Validation Loss: 6.278338432312012\n",
      "Step: 10375/20000 | Training Loss: 6.473258972167969 | Validation Loss: 6.273295879364014\n",
      "Step: 10376/20000 | Training Loss: 5.591911792755127 | Validation Loss: 6.27247953414917\n",
      "Step: 10377/20000 | Training Loss: 5.139490604400635 | Validation Loss: 6.289127349853516\n",
      "Step: 10378/20000 | Training Loss: 5.595566272735596 | Validation Loss: 6.2869553565979\n",
      "Step: 10379/20000 | Training Loss: 6.221928119659424 | Validation Loss: 6.277461051940918\n",
      "Step: 10380/20000 | Training Loss: 4.6429877281188965 | Validation Loss: 6.315056324005127\n",
      "Step: 10381/20000 | Training Loss: 5.245395660400391 | Validation Loss: 6.307059288024902\n",
      "Step: 10382/20000 | Training Loss: 5.262608528137207 | Validation Loss: 6.309753894805908\n",
      "Step: 10383/20000 | Training Loss: 5.442044258117676 | Validation Loss: 6.302383899688721\n",
      "Step: 10384/20000 | Training Loss: 5.670027732849121 | Validation Loss: 6.288055419921875\n",
      "Step: 10385/20000 | Training Loss: 5.387576103210449 | Validation Loss: 6.286798000335693\n",
      "Step: 10386/20000 | Training Loss: 5.769718647003174 | Validation Loss: 6.28028678894043\n",
      "Step: 10387/20000 | Training Loss: 6.277493476867676 | Validation Loss: 6.278377056121826\n",
      "Step: 10388/20000 | Training Loss: 5.079463481903076 | Validation Loss: 6.294708251953125\n",
      "Step: 10389/20000 | Training Loss: 5.364484786987305 | Validation Loss: 6.306629657745361\n",
      "Step: 10390/20000 | Training Loss: 6.644946098327637 | Validation Loss: 6.293880462646484\n",
      "Step: 10391/20000 | Training Loss: 5.394302845001221 | Validation Loss: 6.285776615142822\n",
      "Step: 10392/20000 | Training Loss: 5.732938289642334 | Validation Loss: 6.28417444229126\n",
      "Step: 10393/20000 | Training Loss: 5.334686756134033 | Validation Loss: 6.2908782958984375\n",
      "Step: 10394/20000 | Training Loss: 5.640872001647949 | Validation Loss: 6.303175449371338\n",
      "Step: 10395/20000 | Training Loss: 5.94121789932251 | Validation Loss: 6.283279895782471\n",
      "Step: 10396/20000 | Training Loss: 5.204200744628906 | Validation Loss: 6.315739154815674\n",
      "Step: 10397/20000 | Training Loss: 5.0661725997924805 | Validation Loss: 6.307758808135986\n",
      "Step: 10398/20000 | Training Loss: 5.525102615356445 | Validation Loss: 6.302762508392334\n",
      "Step: 10399/20000 | Training Loss: 6.136542320251465 | Validation Loss: 6.277894020080566\n",
      "Step: 10400/20000 | Training Loss: 5.657782077789307 | Validation Loss: 6.291762351989746\n",
      "Step: 10401/20000 | Training Loss: 5.719388961791992 | Validation Loss: 6.30303955078125\n",
      "Step: 10402/20000 | Training Loss: 5.764122009277344 | Validation Loss: 6.276149749755859\n",
      "Step: 10403/20000 | Training Loss: 5.197057247161865 | Validation Loss: 6.284979820251465\n",
      "Step: 10404/20000 | Training Loss: 5.74824333190918 | Validation Loss: 6.2753472328186035\n",
      "Step: 10405/20000 | Training Loss: 5.45628547668457 | Validation Loss: 6.2890944480896\n",
      "Step: 10406/20000 | Training Loss: 5.667900562286377 | Validation Loss: 6.295734405517578\n",
      "Step: 10407/20000 | Training Loss: 5.981997966766357 | Validation Loss: 6.284005165100098\n",
      "Step: 10408/20000 | Training Loss: 5.652316570281982 | Validation Loss: 6.432656764984131\n",
      "Step: 10409/20000 | Training Loss: 5.5464959144592285 | Validation Loss: 6.29964542388916\n",
      "Step: 10410/20000 | Training Loss: 5.62114953994751 | Validation Loss: 6.291275501251221\n",
      "Step: 10411/20000 | Training Loss: 5.466283321380615 | Validation Loss: 6.2923712730407715\n",
      "Step: 10412/20000 | Training Loss: 5.020083427429199 | Validation Loss: 6.307614803314209\n",
      "Step: 10413/20000 | Training Loss: 5.135469436645508 | Validation Loss: 6.31264066696167\n",
      "Step: 10414/20000 | Training Loss: 5.732317924499512 | Validation Loss: 6.282958030700684\n",
      "Step: 10415/20000 | Training Loss: 5.757820129394531 | Validation Loss: 6.286184310913086\n",
      "Step: 10416/20000 | Training Loss: 6.106306552886963 | Validation Loss: 6.2701096534729\n",
      "Step: 10417/20000 | Training Loss: 5.538855075836182 | Validation Loss: 6.277806758880615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10418/20000 | Training Loss: 4.908864974975586 | Validation Loss: 6.2901458740234375\n",
      "Step: 10419/20000 | Training Loss: 5.803831100463867 | Validation Loss: 6.278071403503418\n",
      "Step: 10420/20000 | Training Loss: 5.628860950469971 | Validation Loss: 6.269072532653809\n",
      "Step: 10421/20000 | Training Loss: 5.759814262390137 | Validation Loss: 6.263304710388184\n",
      "Step: 10422/20000 | Training Loss: 5.609277248382568 | Validation Loss: 6.267226696014404\n",
      "Step: 10423/20000 | Training Loss: 5.180486679077148 | Validation Loss: 6.280616760253906\n",
      "Step: 10424/20000 | Training Loss: 5.630953311920166 | Validation Loss: 6.310789108276367\n",
      "Step: 10425/20000 | Training Loss: 6.270550727844238 | Validation Loss: 6.258435249328613\n",
      "Step: 10426/20000 | Training Loss: 5.676966190338135 | Validation Loss: 6.260519981384277\n",
      "Step: 10427/20000 | Training Loss: 5.748951435089111 | Validation Loss: 6.259109973907471\n",
      "Step: 10428/20000 | Training Loss: 5.542107582092285 | Validation Loss: 6.26997184753418\n",
      "Step: 10429/20000 | Training Loss: 5.1145548820495605 | Validation Loss: 6.2842230796813965\n",
      "Step: 10430/20000 | Training Loss: 5.709278583526611 | Validation Loss: 6.264054775238037\n",
      "Step: 10431/20000 | Training Loss: 5.500479221343994 | Validation Loss: 6.271285533905029\n",
      "Step: 10432/20000 | Training Loss: 5.4698028564453125 | Validation Loss: 6.283483028411865\n",
      "Step: 10433/20000 | Training Loss: 6.509355068206787 | Validation Loss: 6.285338401794434\n",
      "Step: 10434/20000 | Training Loss: 5.206204414367676 | Validation Loss: 6.291750907897949\n",
      "Step: 10435/20000 | Training Loss: 5.679764270782471 | Validation Loss: 6.273643970489502\n",
      "Step: 10436/20000 | Training Loss: 5.627137184143066 | Validation Loss: 6.287421226501465\n",
      "Step: 10437/20000 | Training Loss: 5.885049819946289 | Validation Loss: 6.279471397399902\n",
      "Step: 10438/20000 | Training Loss: 5.70373010635376 | Validation Loss: 6.280208587646484\n",
      "Step: 10439/20000 | Training Loss: 6.0934224128723145 | Validation Loss: 6.282148838043213\n",
      "Step: 10440/20000 | Training Loss: 5.4921441078186035 | Validation Loss: 6.277992248535156\n",
      "Step: 10441/20000 | Training Loss: 5.979893684387207 | Validation Loss: 6.2687530517578125\n",
      "Step: 10442/20000 | Training Loss: 5.8372907638549805 | Validation Loss: 6.26877498626709\n",
      "Step: 10443/20000 | Training Loss: 5.398792266845703 | Validation Loss: 6.270275115966797\n",
      "Step: 10444/20000 | Training Loss: 5.7709245681762695 | Validation Loss: 6.274374485015869\n",
      "Step: 10445/20000 | Training Loss: 5.285611629486084 | Validation Loss: 6.290407180786133\n",
      "Step: 10446/20000 | Training Loss: 5.623187065124512 | Validation Loss: 6.289976596832275\n",
      "Step: 10447/20000 | Training Loss: 5.792685508728027 | Validation Loss: 6.290831565856934\n",
      "Step: 10448/20000 | Training Loss: 5.431286811828613 | Validation Loss: 6.294408321380615\n",
      "Step: 10449/20000 | Training Loss: 6.096408843994141 | Validation Loss: 6.288942813873291\n",
      "Step: 10450/20000 | Training Loss: 5.852288246154785 | Validation Loss: 6.286600589752197\n",
      "Step: 10451/20000 | Training Loss: 5.318520545959473 | Validation Loss: 6.293362140655518\n",
      "Step: 10452/20000 | Training Loss: 5.131674289703369 | Validation Loss: 6.304541110992432\n",
      "Step: 10453/20000 | Training Loss: 4.954145431518555 | Validation Loss: 6.319362640380859\n",
      "Step: 10454/20000 | Training Loss: 6.04851770401001 | Validation Loss: 6.310895919799805\n",
      "Step: 10455/20000 | Training Loss: 5.814801216125488 | Validation Loss: 6.2874650955200195\n",
      "Step: 10456/20000 | Training Loss: 5.217365264892578 | Validation Loss: 6.297823905944824\n",
      "Step: 10457/20000 | Training Loss: 6.204371452331543 | Validation Loss: 6.283219337463379\n",
      "Step: 10458/20000 | Training Loss: 5.573293209075928 | Validation Loss: 6.288925647735596\n",
      "Step: 10459/20000 | Training Loss: 5.658864498138428 | Validation Loss: 6.281050205230713\n",
      "Step: 10460/20000 | Training Loss: 5.589569091796875 | Validation Loss: 6.280147552490234\n",
      "Step: 10461/20000 | Training Loss: 5.879555702209473 | Validation Loss: 6.2800188064575195\n",
      "Step: 10462/20000 | Training Loss: 5.700331211090088 | Validation Loss: 6.27853536605835\n",
      "Step: 10463/20000 | Training Loss: 4.824981212615967 | Validation Loss: 6.314541339874268\n",
      "Step: 10464/20000 | Training Loss: 5.399359703063965 | Validation Loss: 6.31224250793457\n",
      "Step: 10465/20000 | Training Loss: 5.5435614585876465 | Validation Loss: 6.299476146697998\n",
      "Step: 10466/20000 | Training Loss: 5.680856704711914 | Validation Loss: 6.288880825042725\n",
      "Step: 10467/20000 | Training Loss: 6.169076442718506 | Validation Loss: 6.279819488525391\n",
      "Step: 10468/20000 | Training Loss: 5.248865127563477 | Validation Loss: 6.282825469970703\n",
      "Step: 10469/20000 | Training Loss: 5.857224941253662 | Validation Loss: 6.276686191558838\n",
      "Step: 10470/20000 | Training Loss: 6.046159744262695 | Validation Loss: 6.276908874511719\n",
      "Step: 10471/20000 | Training Loss: 6.122558116912842 | Validation Loss: 6.282476902008057\n",
      "Step: 10472/20000 | Training Loss: 5.150712966918945 | Validation Loss: 6.292001724243164\n",
      "Step: 10473/20000 | Training Loss: 5.468952655792236 | Validation Loss: 6.293264389038086\n",
      "Step: 10474/20000 | Training Loss: 5.7029852867126465 | Validation Loss: 6.281722545623779\n",
      "Step: 10475/20000 | Training Loss: 5.433539390563965 | Validation Loss: 6.287599563598633\n",
      "Step: 10476/20000 | Training Loss: 5.749072074890137 | Validation Loss: 6.301349639892578\n",
      "Step: 10477/20000 | Training Loss: 5.305291652679443 | Validation Loss: 6.313220500946045\n",
      "Step: 10478/20000 | Training Loss: 5.830324649810791 | Validation Loss: 6.2863569259643555\n",
      "Step: 10479/20000 | Training Loss: 6.160186767578125 | Validation Loss: 6.276874542236328\n",
      "Step: 10480/20000 | Training Loss: 5.451712131500244 | Validation Loss: 6.308364391326904\n",
      "Step: 10481/20000 | Training Loss: 5.415926933288574 | Validation Loss: 6.299374103546143\n",
      "Step: 10482/20000 | Training Loss: 5.498376846313477 | Validation Loss: 6.2921061515808105\n",
      "Step: 10483/20000 | Training Loss: 5.422007083892822 | Validation Loss: 6.288128852844238\n",
      "Step: 10484/20000 | Training Loss: 5.75671911239624 | Validation Loss: 6.279672622680664\n",
      "Step: 10485/20000 | Training Loss: 5.620484828948975 | Validation Loss: 6.277035236358643\n",
      "Step: 10486/20000 | Training Loss: 4.930573463439941 | Validation Loss: 6.3224711418151855\n",
      "Step: 10487/20000 | Training Loss: 6.032443046569824 | Validation Loss: 6.280418872833252\n",
      "Step: 10488/20000 | Training Loss: 5.5417985916137695 | Validation Loss: 6.283867835998535\n",
      "Step: 10489/20000 | Training Loss: 5.242469787597656 | Validation Loss: 6.2969818115234375\n",
      "Step: 10490/20000 | Training Loss: 5.498222827911377 | Validation Loss: 6.290417194366455\n",
      "Step: 10491/20000 | Training Loss: 5.587894439697266 | Validation Loss: 6.290642261505127\n",
      "Step: 10492/20000 | Training Loss: 4.95376443862915 | Validation Loss: 6.304807186126709\n",
      "Step: 10493/20000 | Training Loss: 5.744887828826904 | Validation Loss: 6.280627250671387\n",
      "Step: 10494/20000 | Training Loss: 5.939940452575684 | Validation Loss: 6.281745433807373\n",
      "Step: 10495/20000 | Training Loss: 5.8824357986450195 | Validation Loss: 6.281522274017334\n",
      "Step: 10496/20000 | Training Loss: 5.849266052246094 | Validation Loss: 6.278238773345947\n",
      "Step: 10497/20000 | Training Loss: 5.432011604309082 | Validation Loss: 6.284434795379639\n",
      "Step: 10498/20000 | Training Loss: 6.01519250869751 | Validation Loss: 6.271682262420654\n",
      "Step: 10499/20000 | Training Loss: 5.283724784851074 | Validation Loss: 6.288159370422363\n",
      "Step: 10500/20000 | Training Loss: 5.847371578216553 | Validation Loss: 6.281026363372803\n",
      "Step: 10501/20000 | Training Loss: 5.507376670837402 | Validation Loss: 6.2850446701049805\n",
      "Step: 10502/20000 | Training Loss: 5.311105251312256 | Validation Loss: 6.297118663787842\n",
      "Step: 10503/20000 | Training Loss: 6.212974548339844 | Validation Loss: 6.291926860809326\n",
      "Step: 10504/20000 | Training Loss: 5.900031566619873 | Validation Loss: 6.281407356262207\n",
      "Step: 10505/20000 | Training Loss: 5.730965614318848 | Validation Loss: 6.278426170349121\n",
      "Step: 10506/20000 | Training Loss: 5.778158187866211 | Validation Loss: 6.276317119598389\n",
      "Step: 10507/20000 | Training Loss: 5.396105766296387 | Validation Loss: 6.292731761932373\n",
      "Step: 10508/20000 | Training Loss: 6.075771331787109 | Validation Loss: 6.283299922943115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10509/20000 | Training Loss: 5.884615421295166 | Validation Loss: 6.274389743804932\n",
      "Step: 10510/20000 | Training Loss: 5.4999566078186035 | Validation Loss: 6.27646017074585\n",
      "Step: 10511/20000 | Training Loss: 5.315322399139404 | Validation Loss: 6.284076690673828\n",
      "Step: 10512/20000 | Training Loss: 5.70214319229126 | Validation Loss: 6.279187202453613\n",
      "Step: 10513/20000 | Training Loss: 6.0215654373168945 | Validation Loss: 6.271651268005371\n",
      "Step: 10514/20000 | Training Loss: 5.984991073608398 | Validation Loss: 6.268319606781006\n",
      "Step: 10515/20000 | Training Loss: 5.767414569854736 | Validation Loss: 6.275357723236084\n",
      "Step: 10516/20000 | Training Loss: 5.269726276397705 | Validation Loss: 6.286497592926025\n",
      "Step: 10517/20000 | Training Loss: 5.724367618560791 | Validation Loss: 6.287006378173828\n",
      "Step: 10518/20000 | Training Loss: 5.799556255340576 | Validation Loss: 6.28331184387207\n",
      "Step: 10519/20000 | Training Loss: 4.527338981628418 | Validation Loss: 6.344598293304443\n",
      "Step: 10520/20000 | Training Loss: 5.649012565612793 | Validation Loss: 6.3046393394470215\n",
      "Step: 10521/20000 | Training Loss: 5.69744348526001 | Validation Loss: 6.300711154937744\n",
      "Step: 10522/20000 | Training Loss: 5.356761455535889 | Validation Loss: 6.297976970672607\n",
      "Step: 10523/20000 | Training Loss: 6.292794227600098 | Validation Loss: 6.288074493408203\n",
      "Step: 10524/20000 | Training Loss: 5.674888610839844 | Validation Loss: 6.272045135498047\n",
      "Step: 10525/20000 | Training Loss: 5.314742088317871 | Validation Loss: 6.289780616760254\n",
      "Step: 10526/20000 | Training Loss: 6.114494323730469 | Validation Loss: 6.276512622833252\n",
      "Step: 10527/20000 | Training Loss: 5.823383331298828 | Validation Loss: 6.279777526855469\n",
      "Step: 10528/20000 | Training Loss: 5.9144206047058105 | Validation Loss: 6.270471096038818\n",
      "Step: 10529/20000 | Training Loss: 5.772883415222168 | Validation Loss: 6.270838260650635\n",
      "Step: 10530/20000 | Training Loss: 5.601509094238281 | Validation Loss: 6.270618438720703\n",
      "Step: 10531/20000 | Training Loss: 5.678041458129883 | Validation Loss: 6.275815963745117\n",
      "Step: 10532/20000 | Training Loss: 5.5189056396484375 | Validation Loss: 6.284461975097656\n",
      "Step: 10533/20000 | Training Loss: 5.800390243530273 | Validation Loss: 6.277284622192383\n",
      "Step: 10534/20000 | Training Loss: 5.454650402069092 | Validation Loss: 6.2798542976379395\n",
      "Step: 10535/20000 | Training Loss: 5.990194320678711 | Validation Loss: 6.279543876647949\n",
      "Step: 10536/20000 | Training Loss: 6.316393852233887 | Validation Loss: 6.277708053588867\n",
      "Step: 10537/20000 | Training Loss: 5.593054294586182 | Validation Loss: 6.280919551849365\n",
      "Step: 10538/20000 | Training Loss: 5.755953311920166 | Validation Loss: 6.28545618057251\n",
      "Step: 10539/20000 | Training Loss: 5.8258056640625 | Validation Loss: 6.282960414886475\n",
      "Step: 10540/20000 | Training Loss: 5.635638236999512 | Validation Loss: 6.296698570251465\n",
      "Step: 10541/20000 | Training Loss: 6.060817241668701 | Validation Loss: 6.2982330322265625\n",
      "Step: 10542/20000 | Training Loss: 5.322151184082031 | Validation Loss: 6.2972731590271\n",
      "Step: 10543/20000 | Training Loss: 5.913869857788086 | Validation Loss: 6.297642230987549\n",
      "Step: 10544/20000 | Training Loss: 6.521895885467529 | Validation Loss: 6.295162200927734\n",
      "Step: 10545/20000 | Training Loss: 6.069729328155518 | Validation Loss: 6.290554523468018\n",
      "Step: 10546/20000 | Training Loss: 6.331137657165527 | Validation Loss: 6.300151824951172\n",
      "Step: 10547/20000 | Training Loss: 5.926577568054199 | Validation Loss: 6.288573741912842\n",
      "Step: 10548/20000 | Training Loss: 5.858328819274902 | Validation Loss: 6.2872748374938965\n",
      "Step: 10549/20000 | Training Loss: 5.631561756134033 | Validation Loss: 6.2852044105529785\n",
      "Step: 10550/20000 | Training Loss: 5.444209575653076 | Validation Loss: 6.305148601531982\n",
      "Step: 10551/20000 | Training Loss: 5.611509799957275 | Validation Loss: 6.283117771148682\n",
      "Step: 10552/20000 | Training Loss: 5.686608791351318 | Validation Loss: 6.274288177490234\n",
      "Step: 10553/20000 | Training Loss: 5.153576374053955 | Validation Loss: 6.3009257316589355\n",
      "Step: 10554/20000 | Training Loss: 5.95421838760376 | Validation Loss: 6.271476745605469\n",
      "Step: 10555/20000 | Training Loss: 5.892274379730225 | Validation Loss: 6.278475284576416\n",
      "Step: 10556/20000 | Training Loss: 6.141729354858398 | Validation Loss: 6.274811744689941\n",
      "Step: 10557/20000 | Training Loss: 5.350998401641846 | Validation Loss: 6.279606342315674\n",
      "Step: 10558/20000 | Training Loss: 5.294398307800293 | Validation Loss: 6.2985992431640625\n",
      "Step: 10559/20000 | Training Loss: 5.0630574226379395 | Validation Loss: 6.294638633728027\n",
      "Step: 10560/20000 | Training Loss: 5.814732551574707 | Validation Loss: 6.2820611000061035\n",
      "Step: 10561/20000 | Training Loss: 5.370408535003662 | Validation Loss: 6.312260627746582\n",
      "Step: 10562/20000 | Training Loss: 5.759366035461426 | Validation Loss: 6.270012378692627\n",
      "Step: 10563/20000 | Training Loss: 5.808638095855713 | Validation Loss: 6.26814603805542\n",
      "Step: 10564/20000 | Training Loss: 5.5277485847473145 | Validation Loss: 6.279632091522217\n",
      "Step: 10565/20000 | Training Loss: 5.801122188568115 | Validation Loss: 6.281418323516846\n",
      "Step: 10566/20000 | Training Loss: 5.185100555419922 | Validation Loss: 6.294999599456787\n",
      "Step: 10567/20000 | Training Loss: 5.126091957092285 | Validation Loss: 6.310936450958252\n",
      "Step: 10568/20000 | Training Loss: 5.686361789703369 | Validation Loss: 6.293297290802002\n",
      "Step: 10569/20000 | Training Loss: 5.737287998199463 | Validation Loss: 6.292068958282471\n",
      "Step: 10570/20000 | Training Loss: 5.62983512878418 | Validation Loss: 6.287626266479492\n",
      "Step: 10571/20000 | Training Loss: 5.282308578491211 | Validation Loss: 6.295510292053223\n",
      "Step: 10572/20000 | Training Loss: 5.505885601043701 | Validation Loss: 6.295191764831543\n",
      "Step: 10573/20000 | Training Loss: 5.444919109344482 | Validation Loss: 6.292834281921387\n",
      "Step: 10574/20000 | Training Loss: 5.90748405456543 | Validation Loss: 6.283794403076172\n",
      "Step: 10575/20000 | Training Loss: 5.971461772918701 | Validation Loss: 6.271921157836914\n",
      "Step: 10576/20000 | Training Loss: 5.009311676025391 | Validation Loss: 6.295668125152588\n",
      "Step: 10577/20000 | Training Loss: 6.059478759765625 | Validation Loss: 6.2732954025268555\n",
      "Step: 10578/20000 | Training Loss: 5.414724826812744 | Validation Loss: 6.2805495262146\n",
      "Step: 10579/20000 | Training Loss: 5.664670467376709 | Validation Loss: 6.274816989898682\n",
      "Step: 10580/20000 | Training Loss: 5.85623025894165 | Validation Loss: 6.272378921508789\n",
      "Step: 10581/20000 | Training Loss: 5.857602596282959 | Validation Loss: 6.295844078063965\n",
      "Step: 10582/20000 | Training Loss: 5.800253391265869 | Validation Loss: 6.308749198913574\n",
      "Step: 10583/20000 | Training Loss: 5.7534403800964355 | Validation Loss: 6.278975009918213\n",
      "Step: 10584/20000 | Training Loss: 5.143177509307861 | Validation Loss: 6.292633533477783\n",
      "Step: 10585/20000 | Training Loss: 6.201422214508057 | Validation Loss: 6.277129173278809\n",
      "Step: 10586/20000 | Training Loss: 5.332640647888184 | Validation Loss: 6.272467136383057\n",
      "Step: 10587/20000 | Training Loss: 5.931817054748535 | Validation Loss: 6.265598297119141\n",
      "Step: 10588/20000 | Training Loss: 5.513594627380371 | Validation Loss: 6.2722272872924805\n",
      "Step: 10589/20000 | Training Loss: 5.744986057281494 | Validation Loss: 6.272277355194092\n",
      "Step: 10590/20000 | Training Loss: 5.251967430114746 | Validation Loss: 6.284193515777588\n",
      "Step: 10591/20000 | Training Loss: 6.068901062011719 | Validation Loss: 6.264934062957764\n",
      "Step: 10592/20000 | Training Loss: 5.767890930175781 | Validation Loss: 6.2618327140808105\n",
      "Step: 10593/20000 | Training Loss: 6.104856967926025 | Validation Loss: 6.262112617492676\n",
      "Step: 10594/20000 | Training Loss: 5.971968173980713 | Validation Loss: 6.300921440124512\n",
      "Step: 10595/20000 | Training Loss: 5.769073009490967 | Validation Loss: 6.266939640045166\n",
      "Step: 10596/20000 | Training Loss: 5.92695951461792 | Validation Loss: 6.268868446350098\n",
      "Step: 10597/20000 | Training Loss: 5.289600849151611 | Validation Loss: 6.276752471923828\n",
      "Step: 10598/20000 | Training Loss: 6.077602386474609 | Validation Loss: 6.268213272094727\n",
      "Step: 10599/20000 | Training Loss: 5.444027423858643 | Validation Loss: 6.284791946411133\n",
      "Step: 10600/20000 | Training Loss: 5.830511569976807 | Validation Loss: 6.28179931640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10601/20000 | Training Loss: 5.601198673248291 | Validation Loss: 6.292696952819824\n",
      "Step: 10602/20000 | Training Loss: 5.994409084320068 | Validation Loss: 6.28103494644165\n",
      "Step: 10603/20000 | Training Loss: 5.562344551086426 | Validation Loss: 6.274105548858643\n",
      "Step: 10604/20000 | Training Loss: 5.869498252868652 | Validation Loss: 6.27333402633667\n",
      "Step: 10605/20000 | Training Loss: 6.074211120605469 | Validation Loss: 6.272628307342529\n",
      "Step: 10606/20000 | Training Loss: 5.937117099761963 | Validation Loss: 6.276615619659424\n",
      "Step: 10607/20000 | Training Loss: 6.149474143981934 | Validation Loss: 6.272722244262695\n",
      "Step: 10608/20000 | Training Loss: 5.202827453613281 | Validation Loss: 6.289615154266357\n",
      "Step: 10609/20000 | Training Loss: 5.3923516273498535 | Validation Loss: 6.300896167755127\n",
      "Step: 10610/20000 | Training Loss: 5.176161766052246 | Validation Loss: 6.312606334686279\n",
      "Step: 10611/20000 | Training Loss: 5.890388488769531 | Validation Loss: 6.296887397766113\n",
      "Step: 10612/20000 | Training Loss: 5.429331302642822 | Validation Loss: 6.293172836303711\n",
      "Step: 10613/20000 | Training Loss: 5.647791862487793 | Validation Loss: 6.296152114868164\n",
      "Step: 10614/20000 | Training Loss: 5.488037586212158 | Validation Loss: 6.300506591796875\n",
      "Step: 10615/20000 | Training Loss: 4.152512073516846 | Validation Loss: 6.411600112915039\n",
      "Step: 10616/20000 | Training Loss: 5.337523937225342 | Validation Loss: 6.343912601470947\n",
      "Step: 10617/20000 | Training Loss: 5.926370620727539 | Validation Loss: 6.295529365539551\n",
      "Step: 10618/20000 | Training Loss: 6.2557759284973145 | Validation Loss: 6.280555725097656\n",
      "Step: 10619/20000 | Training Loss: 6.411104202270508 | Validation Loss: 6.266006946563721\n",
      "Step: 10620/20000 | Training Loss: 5.240739822387695 | Validation Loss: 6.276179790496826\n",
      "Step: 10621/20000 | Training Loss: 6.056795597076416 | Validation Loss: 6.276041030883789\n",
      "Step: 10622/20000 | Training Loss: 5.510626316070557 | Validation Loss: 6.285869121551514\n",
      "Step: 10623/20000 | Training Loss: 5.83110237121582 | Validation Loss: 6.277863502502441\n",
      "Step: 10624/20000 | Training Loss: 5.653405666351318 | Validation Loss: 6.275657653808594\n",
      "Step: 10625/20000 | Training Loss: 5.827860355377197 | Validation Loss: 6.2805609703063965\n",
      "Step: 10626/20000 | Training Loss: 6.122924327850342 | Validation Loss: 6.2863383293151855\n",
      "Step: 10627/20000 | Training Loss: 5.573888778686523 | Validation Loss: 6.285816192626953\n",
      "Step: 10628/20000 | Training Loss: 5.9885663986206055 | Validation Loss: 6.283062934875488\n",
      "Step: 10629/20000 | Training Loss: 6.161861896514893 | Validation Loss: 6.2865471839904785\n",
      "Step: 10630/20000 | Training Loss: 6.116087436676025 | Validation Loss: 6.27744197845459\n",
      "Step: 10631/20000 | Training Loss: 6.102883815765381 | Validation Loss: 6.2805304527282715\n",
      "Step: 10632/20000 | Training Loss: 4.860653877258301 | Validation Loss: 6.32689094543457\n",
      "Step: 10633/20000 | Training Loss: 6.163265705108643 | Validation Loss: 6.288546085357666\n",
      "Step: 10634/20000 | Training Loss: 5.368032932281494 | Validation Loss: 6.284836292266846\n",
      "Step: 10635/20000 | Training Loss: 5.735307216644287 | Validation Loss: 6.283638954162598\n",
      "Step: 10636/20000 | Training Loss: 6.387101173400879 | Validation Loss: 6.285365104675293\n",
      "Step: 10637/20000 | Training Loss: 5.882638454437256 | Validation Loss: 6.281989097595215\n",
      "Step: 10638/20000 | Training Loss: 5.7088165283203125 | Validation Loss: 6.286667346954346\n",
      "Step: 10639/20000 | Training Loss: 5.8318610191345215 | Validation Loss: 6.283348560333252\n",
      "Step: 10640/20000 | Training Loss: 5.3561015129089355 | Validation Loss: 6.304085731506348\n",
      "Step: 10641/20000 | Training Loss: 5.7813825607299805 | Validation Loss: 6.290380001068115\n",
      "Step: 10642/20000 | Training Loss: 5.512059211730957 | Validation Loss: 6.298838138580322\n",
      "Step: 10643/20000 | Training Loss: 5.6938018798828125 | Validation Loss: 6.297769069671631\n",
      "Step: 10644/20000 | Training Loss: 6.145244121551514 | Validation Loss: 6.288643836975098\n",
      "Step: 10645/20000 | Training Loss: 5.452110290527344 | Validation Loss: 6.284277439117432\n",
      "Step: 10646/20000 | Training Loss: 5.529712677001953 | Validation Loss: 6.2833733558654785\n",
      "Step: 10647/20000 | Training Loss: 6.282221794128418 | Validation Loss: 6.277796745300293\n",
      "Step: 10648/20000 | Training Loss: 6.157263278961182 | Validation Loss: 6.271209239959717\n",
      "Step: 10649/20000 | Training Loss: 5.915079116821289 | Validation Loss: 6.283420085906982\n",
      "Step: 10650/20000 | Training Loss: 5.860748767852783 | Validation Loss: 6.317578315734863\n",
      "Step: 10651/20000 | Training Loss: 5.734682083129883 | Validation Loss: 6.28882360458374\n",
      "Step: 10652/20000 | Training Loss: 5.9959492683410645 | Validation Loss: 6.272824764251709\n",
      "Step: 10653/20000 | Training Loss: 5.594324588775635 | Validation Loss: 6.27437686920166\n",
      "Step: 10654/20000 | Training Loss: 5.973689079284668 | Validation Loss: 6.273252964019775\n",
      "Step: 10655/20000 | Training Loss: 5.813939571380615 | Validation Loss: 6.264161586761475\n",
      "Step: 10656/20000 | Training Loss: 5.520804405212402 | Validation Loss: 6.268132209777832\n",
      "Step: 10657/20000 | Training Loss: 5.533727645874023 | Validation Loss: 6.2721967697143555\n",
      "Step: 10658/20000 | Training Loss: 6.275553226470947 | Validation Loss: 6.2777533531188965\n",
      "Step: 10659/20000 | Training Loss: 6.260941028594971 | Validation Loss: 6.269196510314941\n",
      "Step: 10660/20000 | Training Loss: 5.241691589355469 | Validation Loss: 6.283977508544922\n",
      "Step: 10661/20000 | Training Loss: 5.6807861328125 | Validation Loss: 6.288644313812256\n",
      "Step: 10662/20000 | Training Loss: 6.212020397186279 | Validation Loss: 6.288366317749023\n",
      "Step: 10663/20000 | Training Loss: 6.1618499755859375 | Validation Loss: 6.27691125869751\n",
      "Step: 10664/20000 | Training Loss: 5.976783752441406 | Validation Loss: 6.271078109741211\n",
      "Step: 10665/20000 | Training Loss: 5.119270324707031 | Validation Loss: 6.283507347106934\n",
      "Step: 10666/20000 | Training Loss: 5.849359512329102 | Validation Loss: 6.279512405395508\n",
      "Step: 10667/20000 | Training Loss: 6.148348808288574 | Validation Loss: 6.287247180938721\n",
      "Step: 10668/20000 | Training Loss: 5.440781593322754 | Validation Loss: 6.285412788391113\n",
      "Step: 10669/20000 | Training Loss: 5.4445672035217285 | Validation Loss: 6.285775661468506\n",
      "Step: 10670/20000 | Training Loss: 5.639588356018066 | Validation Loss: 6.279229640960693\n",
      "Step: 10671/20000 | Training Loss: 5.855737686157227 | Validation Loss: 6.282443046569824\n",
      "Step: 10672/20000 | Training Loss: 5.394649028778076 | Validation Loss: 6.36881160736084\n",
      "Step: 10673/20000 | Training Loss: 6.232705593109131 | Validation Loss: 6.275409698486328\n",
      "Step: 10674/20000 | Training Loss: 5.6373796463012695 | Validation Loss: 6.293067455291748\n",
      "Step: 10675/20000 | Training Loss: 5.714118003845215 | Validation Loss: 6.271770477294922\n",
      "Step: 10676/20000 | Training Loss: 5.626647472381592 | Validation Loss: 6.270345687866211\n",
      "Step: 10677/20000 | Training Loss: 5.427952289581299 | Validation Loss: 6.269729137420654\n",
      "Step: 10678/20000 | Training Loss: 5.399372100830078 | Validation Loss: 6.281322002410889\n",
      "Step: 10679/20000 | Training Loss: 5.092037200927734 | Validation Loss: 6.296799182891846\n",
      "Step: 10680/20000 | Training Loss: 4.8626933097839355 | Validation Loss: 6.304539680480957\n",
      "Step: 10681/20000 | Training Loss: 5.649795055389404 | Validation Loss: 6.287986755371094\n",
      "Step: 10682/20000 | Training Loss: 5.553915023803711 | Validation Loss: 6.284237384796143\n",
      "Step: 10683/20000 | Training Loss: 4.854966640472412 | Validation Loss: 6.318626403808594\n",
      "Step: 10684/20000 | Training Loss: 6.090226650238037 | Validation Loss: 6.280252456665039\n",
      "Step: 10685/20000 | Training Loss: 6.165675163269043 | Validation Loss: 6.275976181030273\n",
      "Step: 10686/20000 | Training Loss: 5.775702476501465 | Validation Loss: 6.2817463874816895\n",
      "Step: 10687/20000 | Training Loss: 6.0432844161987305 | Validation Loss: 6.292360305786133\n",
      "Step: 10688/20000 | Training Loss: 5.738192558288574 | Validation Loss: 6.272329807281494\n",
      "Step: 10689/20000 | Training Loss: 5.899825572967529 | Validation Loss: 6.2736382484436035\n",
      "Step: 10690/20000 | Training Loss: 5.242171764373779 | Validation Loss: 6.27931022644043\n",
      "Step: 10691/20000 | Training Loss: 5.87033224105835 | Validation Loss: 6.296649932861328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10692/20000 | Training Loss: 5.355832576751709 | Validation Loss: 6.289157390594482\n",
      "Step: 10693/20000 | Training Loss: 5.726739406585693 | Validation Loss: 6.2887725830078125\n",
      "Step: 10694/20000 | Training Loss: 5.956144332885742 | Validation Loss: 6.285572052001953\n",
      "Step: 10695/20000 | Training Loss: 5.816746711730957 | Validation Loss: 6.285782337188721\n",
      "Step: 10696/20000 | Training Loss: 5.076285362243652 | Validation Loss: 6.299558639526367\n",
      "Step: 10697/20000 | Training Loss: 5.7727742195129395 | Validation Loss: 6.286783218383789\n",
      "Step: 10698/20000 | Training Loss: 5.458215236663818 | Validation Loss: 6.301659107208252\n",
      "Step: 10699/20000 | Training Loss: 5.738052845001221 | Validation Loss: 6.290841102600098\n",
      "Step: 10700/20000 | Training Loss: 5.776301860809326 | Validation Loss: 6.286419868469238\n",
      "Step: 10701/20000 | Training Loss: 5.297580718994141 | Validation Loss: 6.303589344024658\n",
      "Step: 10702/20000 | Training Loss: 5.991734981536865 | Validation Loss: 6.291856288909912\n",
      "Step: 10703/20000 | Training Loss: 5.874012470245361 | Validation Loss: 6.2783403396606445\n",
      "Step: 10704/20000 | Training Loss: 6.087824821472168 | Validation Loss: 6.277805328369141\n",
      "Step: 10705/20000 | Training Loss: 5.243385314941406 | Validation Loss: 6.287669658660889\n",
      "Step: 10706/20000 | Training Loss: 5.16984748840332 | Validation Loss: 6.303117752075195\n",
      "Step: 10707/20000 | Training Loss: 4.947819709777832 | Validation Loss: 6.325546741485596\n",
      "Step: 10708/20000 | Training Loss: 5.024043083190918 | Validation Loss: 6.341125011444092\n",
      "Step: 10709/20000 | Training Loss: 5.472799301147461 | Validation Loss: 6.307021617889404\n",
      "Step: 10710/20000 | Training Loss: 5.690235137939453 | Validation Loss: 6.282150745391846\n",
      "Step: 10711/20000 | Training Loss: 5.635103225708008 | Validation Loss: 6.279973983764648\n",
      "Step: 10712/20000 | Training Loss: 5.932500839233398 | Validation Loss: 6.276200294494629\n",
      "Step: 10713/20000 | Training Loss: 5.667684078216553 | Validation Loss: 6.278850078582764\n",
      "Step: 10714/20000 | Training Loss: 5.2021260261535645 | Validation Loss: 6.297560214996338\n",
      "Step: 10715/20000 | Training Loss: 5.3014631271362305 | Validation Loss: 6.295631408691406\n",
      "Step: 10716/20000 | Training Loss: 5.487975120544434 | Validation Loss: 6.307422161102295\n",
      "Step: 10717/20000 | Training Loss: 5.67598295211792 | Validation Loss: 6.282861232757568\n",
      "Step: 10718/20000 | Training Loss: 5.315127849578857 | Validation Loss: 6.288295745849609\n",
      "Step: 10719/20000 | Training Loss: 5.318352222442627 | Validation Loss: 6.289452075958252\n",
      "Step: 10720/20000 | Training Loss: 5.297112464904785 | Validation Loss: 6.292679309844971\n",
      "Step: 10721/20000 | Training Loss: 5.703742504119873 | Validation Loss: 6.2801055908203125\n",
      "Step: 10722/20000 | Training Loss: 5.5670695304870605 | Validation Loss: 6.283642292022705\n",
      "Step: 10723/20000 | Training Loss: 5.006321430206299 | Validation Loss: 6.304837703704834\n",
      "Step: 10724/20000 | Training Loss: 5.7420501708984375 | Validation Loss: 6.292708873748779\n",
      "Step: 10725/20000 | Training Loss: 5.52250862121582 | Validation Loss: 6.286209583282471\n",
      "Step: 10726/20000 | Training Loss: 5.385275840759277 | Validation Loss: 6.29001522064209\n",
      "Step: 10727/20000 | Training Loss: 5.409972190856934 | Validation Loss: 6.297069549560547\n",
      "Step: 10728/20000 | Training Loss: 6.527822971343994 | Validation Loss: 6.283917427062988\n",
      "Step: 10729/20000 | Training Loss: 5.728145122528076 | Validation Loss: 6.277353286743164\n",
      "Step: 10730/20000 | Training Loss: 5.3119683265686035 | Validation Loss: 6.278435230255127\n",
      "Step: 10731/20000 | Training Loss: 5.486968517303467 | Validation Loss: 6.283190727233887\n",
      "Step: 10732/20000 | Training Loss: 6.26984167098999 | Validation Loss: 6.260634899139404\n",
      "Step: 10733/20000 | Training Loss: 5.482786178588867 | Validation Loss: 6.267166614532471\n",
      "Step: 10734/20000 | Training Loss: 6.160702228546143 | Validation Loss: 6.26869010925293\n",
      "Step: 10735/20000 | Training Loss: 5.877896785736084 | Validation Loss: 6.274809837341309\n",
      "Step: 10736/20000 | Training Loss: 6.007200717926025 | Validation Loss: 6.270261287689209\n",
      "Step: 10737/20000 | Training Loss: 6.218663215637207 | Validation Loss: 6.275140285491943\n",
      "Step: 10738/20000 | Training Loss: 5.922140598297119 | Validation Loss: 6.266402721405029\n",
      "Step: 10739/20000 | Training Loss: 5.3767571449279785 | Validation Loss: 6.268611431121826\n",
      "Step: 10740/20000 | Training Loss: 6.2007646560668945 | Validation Loss: 6.277520179748535\n",
      "Step: 10741/20000 | Training Loss: 5.86992883682251 | Validation Loss: 6.262280464172363\n",
      "Step: 10742/20000 | Training Loss: 6.227408409118652 | Validation Loss: 6.265517234802246\n",
      "Step: 10743/20000 | Training Loss: 5.41819429397583 | Validation Loss: 6.261908531188965\n",
      "Step: 10744/20000 | Training Loss: 6.144969940185547 | Validation Loss: 6.2577948570251465\n",
      "Step: 10745/20000 | Training Loss: 5.854673385620117 | Validation Loss: 6.259125232696533\n",
      "Step: 10746/20000 | Training Loss: 4.7636518478393555 | Validation Loss: 6.286993026733398\n",
      "Step: 10747/20000 | Training Loss: 6.165533542633057 | Validation Loss: 6.263425827026367\n",
      "Step: 10748/20000 | Training Loss: 5.824840545654297 | Validation Loss: 6.259917259216309\n",
      "Step: 10749/20000 | Training Loss: 5.5301194190979 | Validation Loss: 6.280655860900879\n",
      "Step: 10750/20000 | Training Loss: 5.648665904998779 | Validation Loss: 6.278231143951416\n",
      "Step: 10751/20000 | Training Loss: 6.186967372894287 | Validation Loss: 6.28513765335083\n",
      "Step: 10752/20000 | Training Loss: 6.061962127685547 | Validation Loss: 6.260150909423828\n",
      "Step: 10753/20000 | Training Loss: 6.128762722015381 | Validation Loss: 6.286696910858154\n",
      "Step: 10754/20000 | Training Loss: 5.98225212097168 | Validation Loss: 6.2735090255737305\n",
      "Step: 10755/20000 | Training Loss: 5.733803749084473 | Validation Loss: 6.267240524291992\n",
      "Step: 10756/20000 | Training Loss: 5.866336345672607 | Validation Loss: 6.300291061401367\n",
      "Step: 10757/20000 | Training Loss: 5.6727519035339355 | Validation Loss: 6.2889018058776855\n",
      "Step: 10758/20000 | Training Loss: 5.870331764221191 | Validation Loss: 6.283115386962891\n",
      "Step: 10759/20000 | Training Loss: 5.461834907531738 | Validation Loss: 6.281842231750488\n",
      "Step: 10760/20000 | Training Loss: 5.438260078430176 | Validation Loss: 6.284173011779785\n",
      "Step: 10761/20000 | Training Loss: 6.081513404846191 | Validation Loss: 6.287487983703613\n",
      "Step: 10762/20000 | Training Loss: 5.467628002166748 | Validation Loss: 6.28798246383667\n",
      "Step: 10763/20000 | Training Loss: 5.091679096221924 | Validation Loss: 6.31019926071167\n",
      "Step: 10764/20000 | Training Loss: 5.852366924285889 | Validation Loss: 6.287907123565674\n",
      "Step: 10765/20000 | Training Loss: 6.217197895050049 | Validation Loss: 6.276288986206055\n",
      "Step: 10766/20000 | Training Loss: 5.8937296867370605 | Validation Loss: 6.265598773956299\n",
      "Step: 10767/20000 | Training Loss: 6.025886535644531 | Validation Loss: 6.295788288116455\n",
      "Step: 10768/20000 | Training Loss: 5.661814212799072 | Validation Loss: 6.269865036010742\n",
      "Step: 10769/20000 | Training Loss: 6.290787220001221 | Validation Loss: 6.270543098449707\n",
      "Step: 10770/20000 | Training Loss: 6.043916702270508 | Validation Loss: 6.26943302154541\n",
      "Step: 10771/20000 | Training Loss: 6.419963359832764 | Validation Loss: 6.2695631980896\n",
      "Step: 10772/20000 | Training Loss: 5.919035911560059 | Validation Loss: 6.266317844390869\n",
      "Step: 10773/20000 | Training Loss: 5.86490535736084 | Validation Loss: 6.26361608505249\n",
      "Step: 10774/20000 | Training Loss: 6.084512233734131 | Validation Loss: 6.2563652992248535\n",
      "Step: 10775/20000 | Training Loss: 5.589208126068115 | Validation Loss: 6.272851943969727\n",
      "Step: 10776/20000 | Training Loss: 5.896692752838135 | Validation Loss: 6.269888877868652\n",
      "Step: 10777/20000 | Training Loss: 6.009728908538818 | Validation Loss: 6.271674633026123\n",
      "Step: 10778/20000 | Training Loss: 5.598447322845459 | Validation Loss: 6.266210079193115\n",
      "Step: 10779/20000 | Training Loss: 6.005037784576416 | Validation Loss: 6.266639232635498\n",
      "Step: 10780/20000 | Training Loss: 6.525357246398926 | Validation Loss: 6.264240264892578\n",
      "Step: 10781/20000 | Training Loss: 5.932794094085693 | Validation Loss: 6.266692161560059\n",
      "Step: 10782/20000 | Training Loss: 5.203741073608398 | Validation Loss: 6.289604663848877\n",
      "Step: 10783/20000 | Training Loss: 5.892586708068848 | Validation Loss: 6.283486843109131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10784/20000 | Training Loss: 5.952221393585205 | Validation Loss: 6.257375717163086\n",
      "Step: 10785/20000 | Training Loss: 5.679624557495117 | Validation Loss: 6.254715442657471\n",
      "%---Saving the model---%\n",
      "Step: 10786/20000 | Training Loss: 5.4871649742126465 | Validation Loss: 6.259748458862305\n",
      "Step: 10787/20000 | Training Loss: 6.111208438873291 | Validation Loss: 6.252424240112305\n",
      "%---Saving the model---%\n",
      "Step: 10788/20000 | Training Loss: 5.8610405921936035 | Validation Loss: 6.257777214050293\n",
      "Step: 10789/20000 | Training Loss: 5.955638408660889 | Validation Loss: 6.262369155883789\n",
      "Step: 10790/20000 | Training Loss: 5.885756969451904 | Validation Loss: 6.276546478271484\n",
      "Step: 10791/20000 | Training Loss: 5.842705726623535 | Validation Loss: 6.269923210144043\n",
      "Step: 10792/20000 | Training Loss: 6.137452125549316 | Validation Loss: 6.257787704467773\n",
      "Step: 10793/20000 | Training Loss: 5.718854904174805 | Validation Loss: 6.254543781280518\n",
      "Step: 10794/20000 | Training Loss: 5.669920444488525 | Validation Loss: 6.257167339324951\n",
      "Step: 10795/20000 | Training Loss: 5.6849045753479 | Validation Loss: 6.267702102661133\n",
      "Step: 10796/20000 | Training Loss: 5.366621971130371 | Validation Loss: 6.280555725097656\n",
      "Step: 10797/20000 | Training Loss: 5.813780784606934 | Validation Loss: 6.27602481842041\n",
      "Step: 10798/20000 | Training Loss: 5.571689605712891 | Validation Loss: 6.26455020904541\n",
      "Step: 10799/20000 | Training Loss: 6.275538444519043 | Validation Loss: 6.254528999328613\n",
      "Step: 10800/20000 | Training Loss: 5.598398685455322 | Validation Loss: 6.2617106437683105\n",
      "Step: 10801/20000 | Training Loss: 5.636202812194824 | Validation Loss: 6.269144058227539\n",
      "Step: 10802/20000 | Training Loss: 5.626181602478027 | Validation Loss: 6.277460098266602\n",
      "Step: 10803/20000 | Training Loss: 4.830704212188721 | Validation Loss: 6.289200305938721\n",
      "Step: 10804/20000 | Training Loss: 5.9172444343566895 | Validation Loss: 6.262620449066162\n",
      "Step: 10805/20000 | Training Loss: 5.757555961608887 | Validation Loss: 6.256432056427002\n",
      "Step: 10806/20000 | Training Loss: 5.379849433898926 | Validation Loss: 6.2730865478515625\n",
      "Step: 10807/20000 | Training Loss: 5.9910969734191895 | Validation Loss: 6.259824275970459\n",
      "Step: 10808/20000 | Training Loss: 6.157079696655273 | Validation Loss: 6.257177829742432\n",
      "Step: 10809/20000 | Training Loss: 5.788216590881348 | Validation Loss: 6.253462791442871\n",
      "Step: 10810/20000 | Training Loss: 5.685482501983643 | Validation Loss: 6.249481678009033\n",
      "%---Saving the model---%\n",
      "Step: 10811/20000 | Training Loss: 5.747519493103027 | Validation Loss: 6.255704879760742\n",
      "Step: 10812/20000 | Training Loss: 5.976196765899658 | Validation Loss: 6.260283946990967\n",
      "Step: 10813/20000 | Training Loss: 6.0491437911987305 | Validation Loss: 6.256992816925049\n",
      "Step: 10814/20000 | Training Loss: 5.4832048416137695 | Validation Loss: 6.263023376464844\n",
      "Step: 10815/20000 | Training Loss: 5.683484077453613 | Validation Loss: 6.258141040802002\n",
      "Step: 10816/20000 | Training Loss: 5.698856830596924 | Validation Loss: 6.269045352935791\n",
      "Step: 10817/20000 | Training Loss: 5.100667476654053 | Validation Loss: 6.274670124053955\n",
      "Step: 10818/20000 | Training Loss: 5.329599857330322 | Validation Loss: 6.2725934982299805\n",
      "Step: 10819/20000 | Training Loss: 5.862677097320557 | Validation Loss: 6.257277011871338\n",
      "Step: 10820/20000 | Training Loss: 6.09883975982666 | Validation Loss: 6.264013290405273\n",
      "Step: 10821/20000 | Training Loss: 5.8658833503723145 | Validation Loss: 6.257837295532227\n",
      "Step: 10822/20000 | Training Loss: 5.194530963897705 | Validation Loss: 6.267725467681885\n",
      "Step: 10823/20000 | Training Loss: 5.552427768707275 | Validation Loss: 6.263307094573975\n",
      "Step: 10824/20000 | Training Loss: 5.195815563201904 | Validation Loss: 6.276991367340088\n",
      "Step: 10825/20000 | Training Loss: 5.526206016540527 | Validation Loss: 6.271963119506836\n",
      "Step: 10826/20000 | Training Loss: 5.941352844238281 | Validation Loss: 6.25568151473999\n",
      "Step: 10827/20000 | Training Loss: 6.05722188949585 | Validation Loss: 6.248074054718018\n",
      "%---Saving the model---%\n",
      "Step: 10828/20000 | Training Loss: 6.104330539703369 | Validation Loss: 6.240481376647949\n",
      "%---Saving the model---%\n",
      "Step: 10829/20000 | Training Loss: 5.993420124053955 | Validation Loss: 6.250258922576904\n",
      "Step: 10830/20000 | Training Loss: 5.232663154602051 | Validation Loss: 6.262989044189453\n",
      "Step: 10831/20000 | Training Loss: 5.348464012145996 | Validation Loss: 6.255482196807861\n",
      "Step: 10832/20000 | Training Loss: 6.297852993011475 | Validation Loss: 6.252651214599609\n",
      "Step: 10833/20000 | Training Loss: 5.812925815582275 | Validation Loss: 6.2508416175842285\n",
      "Step: 10834/20000 | Training Loss: 5.705546855926514 | Validation Loss: 6.255184173583984\n",
      "Step: 10835/20000 | Training Loss: 5.827585697174072 | Validation Loss: 6.253499984741211\n",
      "Step: 10836/20000 | Training Loss: 5.234846115112305 | Validation Loss: 6.2616987228393555\n",
      "Step: 10837/20000 | Training Loss: 6.608116626739502 | Validation Loss: 6.246460437774658\n",
      "Step: 10838/20000 | Training Loss: 5.7469563484191895 | Validation Loss: 6.248209476470947\n",
      "Step: 10839/20000 | Training Loss: 5.895015239715576 | Validation Loss: 6.248218536376953\n",
      "Step: 10840/20000 | Training Loss: 5.670626640319824 | Validation Loss: 6.241873741149902\n",
      "Step: 10841/20000 | Training Loss: 5.252446174621582 | Validation Loss: 6.24842643737793\n",
      "Step: 10842/20000 | Training Loss: 5.521439075469971 | Validation Loss: 6.25362491607666\n",
      "Step: 10843/20000 | Training Loss: 5.9468207359313965 | Validation Loss: 6.2478413581848145\n",
      "Step: 10844/20000 | Training Loss: 5.197683811187744 | Validation Loss: 6.262058258056641\n",
      "Step: 10845/20000 | Training Loss: 5.587644577026367 | Validation Loss: 6.257080554962158\n",
      "Step: 10846/20000 | Training Loss: 5.403166770935059 | Validation Loss: 6.274815082550049\n",
      "Step: 10847/20000 | Training Loss: 5.89877462387085 | Validation Loss: 6.250009536743164\n",
      "Step: 10848/20000 | Training Loss: 5.605966091156006 | Validation Loss: 6.259953498840332\n",
      "Step: 10849/20000 | Training Loss: 5.485110282897949 | Validation Loss: 6.255607604980469\n",
      "Step: 10850/20000 | Training Loss: 5.911368370056152 | Validation Loss: 6.252835273742676\n",
      "Step: 10851/20000 | Training Loss: 5.975090026855469 | Validation Loss: 6.246481418609619\n",
      "Step: 10852/20000 | Training Loss: 6.175395488739014 | Validation Loss: 6.245464324951172\n",
      "Step: 10853/20000 | Training Loss: 5.127066612243652 | Validation Loss: 6.258849143981934\n",
      "Step: 10854/20000 | Training Loss: 4.451885223388672 | Validation Loss: 6.30647087097168\n",
      "Step: 10855/20000 | Training Loss: 6.135951995849609 | Validation Loss: 6.2585062980651855\n",
      "Step: 10856/20000 | Training Loss: 5.635175704956055 | Validation Loss: 6.26194429397583\n",
      "Step: 10857/20000 | Training Loss: 5.756678581237793 | Validation Loss: 6.266399383544922\n",
      "Step: 10858/20000 | Training Loss: 5.254654884338379 | Validation Loss: 6.25363826751709\n",
      "Step: 10859/20000 | Training Loss: 6.093559265136719 | Validation Loss: 6.2394118309021\n",
      "%---Saving the model---%\n",
      "Step: 10860/20000 | Training Loss: 5.792031288146973 | Validation Loss: 6.2428879737854\n",
      "Step: 10861/20000 | Training Loss: 5.634123802185059 | Validation Loss: 6.246636390686035\n",
      "Step: 10862/20000 | Training Loss: 5.575962543487549 | Validation Loss: 6.246474266052246\n",
      "Step: 10863/20000 | Training Loss: 6.236733436584473 | Validation Loss: 6.24284029006958\n",
      "Step: 10864/20000 | Training Loss: 5.38784646987915 | Validation Loss: 6.249858379364014\n",
      "Step: 10865/20000 | Training Loss: 5.379358291625977 | Validation Loss: 6.262100696563721\n",
      "Step: 10866/20000 | Training Loss: 5.47086238861084 | Validation Loss: 6.262356281280518\n",
      "Step: 10867/20000 | Training Loss: 6.30427885055542 | Validation Loss: 6.246334075927734\n",
      "Step: 10868/20000 | Training Loss: 5.958043575286865 | Validation Loss: 6.264552116394043\n",
      "Step: 10869/20000 | Training Loss: 5.31988525390625 | Validation Loss: 6.2599101066589355\n",
      "Step: 10870/20000 | Training Loss: 5.981988906860352 | Validation Loss: 6.255208492279053\n",
      "Step: 10871/20000 | Training Loss: 5.71293830871582 | Validation Loss: 6.258271217346191\n",
      "Step: 10872/20000 | Training Loss: 5.803127288818359 | Validation Loss: 6.260497570037842\n",
      "Step: 10873/20000 | Training Loss: 5.791200160980225 | Validation Loss: 6.254258632659912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10874/20000 | Training Loss: 6.150381565093994 | Validation Loss: 6.257813930511475\n",
      "Step: 10875/20000 | Training Loss: 5.971368789672852 | Validation Loss: 6.2706990242004395\n",
      "Step: 10876/20000 | Training Loss: 6.28732442855835 | Validation Loss: 6.263794898986816\n",
      "Step: 10877/20000 | Training Loss: 5.835357666015625 | Validation Loss: 6.263377666473389\n",
      "Step: 10878/20000 | Training Loss: 6.055538654327393 | Validation Loss: 6.262484073638916\n",
      "Step: 10879/20000 | Training Loss: 5.6258225440979 | Validation Loss: 6.26190710067749\n",
      "Step: 10880/20000 | Training Loss: 5.533100605010986 | Validation Loss: 6.255860805511475\n",
      "Step: 10881/20000 | Training Loss: 5.90877628326416 | Validation Loss: 6.265361309051514\n",
      "Step: 10882/20000 | Training Loss: 6.205242156982422 | Validation Loss: 6.261037349700928\n",
      "Step: 10883/20000 | Training Loss: 5.827335357666016 | Validation Loss: 6.252494812011719\n",
      "Step: 10884/20000 | Training Loss: 5.896160125732422 | Validation Loss: 6.26103401184082\n",
      "Step: 10885/20000 | Training Loss: 5.953073978424072 | Validation Loss: 6.268997669219971\n",
      "Step: 10886/20000 | Training Loss: 5.7745680809021 | Validation Loss: 6.255923748016357\n",
      "Step: 10887/20000 | Training Loss: 5.53584098815918 | Validation Loss: 6.265281677246094\n",
      "Step: 10888/20000 | Training Loss: 5.82553243637085 | Validation Loss: 6.254319667816162\n",
      "Step: 10889/20000 | Training Loss: 5.508471965789795 | Validation Loss: 6.261125564575195\n",
      "Step: 10890/20000 | Training Loss: 5.604619979858398 | Validation Loss: 6.264525413513184\n",
      "Step: 10891/20000 | Training Loss: 5.812775611877441 | Validation Loss: 6.253845691680908\n",
      "Step: 10892/20000 | Training Loss: 5.562175273895264 | Validation Loss: 6.249194145202637\n",
      "Step: 10893/20000 | Training Loss: 6.238312244415283 | Validation Loss: 6.263575553894043\n",
      "Step: 10894/20000 | Training Loss: 5.858628273010254 | Validation Loss: 6.2446208000183105\n",
      "Step: 10895/20000 | Training Loss: 6.354990482330322 | Validation Loss: 6.244915008544922\n",
      "Step: 10896/20000 | Training Loss: 5.654922008514404 | Validation Loss: 6.248596668243408\n",
      "Step: 10897/20000 | Training Loss: 6.06596565246582 | Validation Loss: 6.246843338012695\n",
      "Step: 10898/20000 | Training Loss: 5.859142780303955 | Validation Loss: 6.2518310546875\n",
      "Step: 10899/20000 | Training Loss: 5.871984958648682 | Validation Loss: 6.254168510437012\n",
      "Step: 10900/20000 | Training Loss: 5.851137638092041 | Validation Loss: 6.255002021789551\n",
      "Step: 10901/20000 | Training Loss: 6.69722318649292 | Validation Loss: 6.280485153198242\n",
      "Step: 10902/20000 | Training Loss: 5.214785575866699 | Validation Loss: 6.279568195343018\n",
      "Step: 10903/20000 | Training Loss: 6.462714195251465 | Validation Loss: 6.263564586639404\n",
      "Step: 10904/20000 | Training Loss: 5.512138843536377 | Validation Loss: 6.265102386474609\n",
      "Step: 10905/20000 | Training Loss: 6.484407901763916 | Validation Loss: 6.2704267501831055\n",
      "Step: 10906/20000 | Training Loss: 5.560643196105957 | Validation Loss: 6.264537811279297\n",
      "Step: 10907/20000 | Training Loss: 5.604502201080322 | Validation Loss: 6.263003826141357\n",
      "Step: 10908/20000 | Training Loss: 5.187365531921387 | Validation Loss: 6.283676624298096\n",
      "Step: 10909/20000 | Training Loss: 5.512883186340332 | Validation Loss: 6.2817606925964355\n",
      "Step: 10910/20000 | Training Loss: 6.551730632781982 | Validation Loss: 6.27643346786499\n",
      "Step: 10911/20000 | Training Loss: 5.5444488525390625 | Validation Loss: 6.273227691650391\n",
      "Step: 10912/20000 | Training Loss: 5.262554168701172 | Validation Loss: 6.275099754333496\n",
      "Step: 10913/20000 | Training Loss: 5.592116832733154 | Validation Loss: 6.275498867034912\n",
      "Step: 10914/20000 | Training Loss: 5.418573379516602 | Validation Loss: 6.275926113128662\n",
      "Step: 10915/20000 | Training Loss: 6.1189470291137695 | Validation Loss: 6.2598347663879395\n",
      "Step: 10916/20000 | Training Loss: 5.7702412605285645 | Validation Loss: 6.2592082023620605\n",
      "Step: 10917/20000 | Training Loss: 5.62269926071167 | Validation Loss: 6.257617473602295\n",
      "Step: 10918/20000 | Training Loss: 5.904233932495117 | Validation Loss: 6.261511325836182\n",
      "Step: 10919/20000 | Training Loss: 6.294267654418945 | Validation Loss: 6.26112699508667\n",
      "Step: 10920/20000 | Training Loss: 5.769746780395508 | Validation Loss: 6.321945667266846\n",
      "Step: 10921/20000 | Training Loss: 6.11362886428833 | Validation Loss: 6.378912925720215\n",
      "Step: 10922/20000 | Training Loss: 5.731774806976318 | Validation Loss: 6.283289909362793\n",
      "Step: 10923/20000 | Training Loss: 6.103975296020508 | Validation Loss: 6.273653507232666\n",
      "Step: 10924/20000 | Training Loss: 5.777108192443848 | Validation Loss: 6.266073226928711\n",
      "Step: 10925/20000 | Training Loss: 5.953832149505615 | Validation Loss: 6.2657999992370605\n",
      "Step: 10926/20000 | Training Loss: 5.809787750244141 | Validation Loss: 6.262495994567871\n",
      "Step: 10927/20000 | Training Loss: 6.226572036743164 | Validation Loss: 6.2689337730407715\n",
      "Step: 10928/20000 | Training Loss: 6.11191987991333 | Validation Loss: 6.266350746154785\n",
      "Step: 10929/20000 | Training Loss: 5.640193939208984 | Validation Loss: 6.258785247802734\n",
      "Step: 10930/20000 | Training Loss: 5.0622944831848145 | Validation Loss: 6.2805328369140625\n",
      "Step: 10931/20000 | Training Loss: 6.287943363189697 | Validation Loss: 6.269768238067627\n",
      "Step: 10932/20000 | Training Loss: 5.893918037414551 | Validation Loss: 6.26303243637085\n",
      "Step: 10933/20000 | Training Loss: 5.378347396850586 | Validation Loss: 6.261388301849365\n",
      "Step: 10934/20000 | Training Loss: 5.451169013977051 | Validation Loss: 6.264402389526367\n",
      "Step: 10935/20000 | Training Loss: 5.714616298675537 | Validation Loss: 6.257100582122803\n",
      "Step: 10936/20000 | Training Loss: 5.540453910827637 | Validation Loss: 6.2553863525390625\n",
      "Step: 10937/20000 | Training Loss: 5.633228778839111 | Validation Loss: 6.257951259613037\n",
      "Step: 10938/20000 | Training Loss: 5.547237873077393 | Validation Loss: 6.255456924438477\n",
      "Step: 10939/20000 | Training Loss: 5.898038864135742 | Validation Loss: 6.249833583831787\n",
      "Step: 10940/20000 | Training Loss: 6.492900848388672 | Validation Loss: 6.249952793121338\n",
      "Step: 10941/20000 | Training Loss: 5.895434856414795 | Validation Loss: 6.269511699676514\n",
      "Step: 10942/20000 | Training Loss: 6.099756717681885 | Validation Loss: 6.255446434020996\n",
      "Step: 10943/20000 | Training Loss: 5.545471668243408 | Validation Loss: 6.266602516174316\n",
      "Step: 10944/20000 | Training Loss: 5.549330234527588 | Validation Loss: 6.276830196380615\n",
      "Step: 10945/20000 | Training Loss: 5.523282527923584 | Validation Loss: 6.275131702423096\n",
      "Step: 10946/20000 | Training Loss: 5.247959136962891 | Validation Loss: 6.276569843292236\n",
      "Step: 10947/20000 | Training Loss: 5.838983058929443 | Validation Loss: 6.262806415557861\n",
      "Step: 10948/20000 | Training Loss: 6.360694885253906 | Validation Loss: 6.250049114227295\n",
      "Step: 10949/20000 | Training Loss: 6.2422709465026855 | Validation Loss: 6.237184524536133\n",
      "%---Saving the model---%\n",
      "Step: 10950/20000 | Training Loss: 6.706352710723877 | Validation Loss: 6.245422840118408\n",
      "Step: 10951/20000 | Training Loss: 6.182334899902344 | Validation Loss: 6.254519462585449\n",
      "Step: 10952/20000 | Training Loss: 6.200447082519531 | Validation Loss: 6.231674671173096\n",
      "%---Saving the model---%\n",
      "Step: 10953/20000 | Training Loss: 5.96815299987793 | Validation Loss: 6.231631755828857\n",
      "%---Saving the model---%\n",
      "Step: 10954/20000 | Training Loss: 5.837725639343262 | Validation Loss: 6.238035202026367\n",
      "Step: 10955/20000 | Training Loss: 5.672693729400635 | Validation Loss: 6.233506679534912\n",
      "Step: 10956/20000 | Training Loss: 5.131630897521973 | Validation Loss: 6.263769149780273\n",
      "Step: 10957/20000 | Training Loss: 5.765336036682129 | Validation Loss: 6.247098445892334\n",
      "Step: 10958/20000 | Training Loss: 5.742559432983398 | Validation Loss: 6.248427391052246\n",
      "Step: 10959/20000 | Training Loss: 5.503324508666992 | Validation Loss: 6.25827693939209\n",
      "Step: 10960/20000 | Training Loss: 5.874617099761963 | Validation Loss: 6.25130033493042\n",
      "Step: 10961/20000 | Training Loss: 5.328261375427246 | Validation Loss: 6.259228706359863\n",
      "Step: 10962/20000 | Training Loss: 5.886409282684326 | Validation Loss: 6.261075973510742\n",
      "Step: 10963/20000 | Training Loss: 5.9184088706970215 | Validation Loss: 6.257937908172607\n",
      "Step: 10964/20000 | Training Loss: 6.055157661437988 | Validation Loss: 6.2576212882995605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 10965/20000 | Training Loss: 5.79731559753418 | Validation Loss: 6.290469169616699\n",
      "Step: 10966/20000 | Training Loss: 5.972685813903809 | Validation Loss: 6.2710041999816895\n",
      "Step: 10967/20000 | Training Loss: 5.97210693359375 | Validation Loss: 6.253302574157715\n",
      "Step: 10968/20000 | Training Loss: 6.147520065307617 | Validation Loss: 6.2506608963012695\n",
      "Step: 10969/20000 | Training Loss: 5.772219181060791 | Validation Loss: 6.265109539031982\n",
      "Step: 10970/20000 | Training Loss: 5.535101413726807 | Validation Loss: 6.264286041259766\n",
      "Step: 10971/20000 | Training Loss: 5.6436872482299805 | Validation Loss: 6.261476039886475\n",
      "Step: 10972/20000 | Training Loss: 5.219757556915283 | Validation Loss: 6.271806716918945\n",
      "Step: 10973/20000 | Training Loss: 6.495218753814697 | Validation Loss: 6.257348537445068\n",
      "Step: 10974/20000 | Training Loss: 5.884106636047363 | Validation Loss: 6.271995544433594\n",
      "Step: 10975/20000 | Training Loss: 5.737949848175049 | Validation Loss: 6.267213344573975\n",
      "Step: 10976/20000 | Training Loss: 5.653680801391602 | Validation Loss: 6.280749320983887\n",
      "Step: 10977/20000 | Training Loss: 5.374145030975342 | Validation Loss: 6.2733378410339355\n",
      "Step: 10978/20000 | Training Loss: 6.5873494148254395 | Validation Loss: 6.262519836425781\n",
      "Step: 10979/20000 | Training Loss: 5.667648792266846 | Validation Loss: 6.255103588104248\n",
      "Step: 10980/20000 | Training Loss: 5.689923286437988 | Validation Loss: 6.281963348388672\n",
      "Step: 10981/20000 | Training Loss: 5.661497116088867 | Validation Loss: 6.2852935791015625\n",
      "Step: 10982/20000 | Training Loss: 5.732480049133301 | Validation Loss: 6.2789716720581055\n",
      "Step: 10983/20000 | Training Loss: 5.566967010498047 | Validation Loss: 6.287062644958496\n",
      "Step: 10984/20000 | Training Loss: 5.619657039642334 | Validation Loss: 6.290902137756348\n",
      "Step: 10985/20000 | Training Loss: 5.603935718536377 | Validation Loss: 6.28494930267334\n",
      "Step: 10986/20000 | Training Loss: 5.408659934997559 | Validation Loss: 6.2878336906433105\n",
      "Step: 10987/20000 | Training Loss: 6.220800876617432 | Validation Loss: 6.271810054779053\n",
      "Step: 10988/20000 | Training Loss: 5.995201587677002 | Validation Loss: 6.274191379547119\n",
      "Step: 10989/20000 | Training Loss: 5.617384910583496 | Validation Loss: 6.275148868560791\n",
      "Step: 10990/20000 | Training Loss: 5.7003068923950195 | Validation Loss: 6.257321834564209\n",
      "Step: 10991/20000 | Training Loss: 6.365564346313477 | Validation Loss: 6.263521671295166\n",
      "Step: 10992/20000 | Training Loss: 6.0092267990112305 | Validation Loss: 6.253270149230957\n",
      "Step: 10993/20000 | Training Loss: 5.879098892211914 | Validation Loss: 6.252614974975586\n",
      "Step: 10994/20000 | Training Loss: 5.221677303314209 | Validation Loss: 6.265537738800049\n",
      "Step: 10995/20000 | Training Loss: 6.415031433105469 | Validation Loss: 6.2563910484313965\n",
      "Step: 10996/20000 | Training Loss: 5.7056989669799805 | Validation Loss: 6.262413501739502\n",
      "Step: 10997/20000 | Training Loss: 5.601261138916016 | Validation Loss: 6.2658467292785645\n",
      "Step: 10998/20000 | Training Loss: 5.446456432342529 | Validation Loss: 6.274801254272461\n",
      "Step: 10999/20000 | Training Loss: 5.661231994628906 | Validation Loss: 6.266661167144775\n",
      "Step: 11000/20000 | Training Loss: 5.549584865570068 | Validation Loss: 6.262439727783203\n",
      "Step: 11001/20000 | Training Loss: 6.471072196960449 | Validation Loss: 6.266495227813721\n",
      "Step: 11002/20000 | Training Loss: 5.575103282928467 | Validation Loss: 6.2706828117370605\n",
      "Step: 11003/20000 | Training Loss: 6.076467514038086 | Validation Loss: 6.286796569824219\n",
      "Step: 11004/20000 | Training Loss: 6.194666385650635 | Validation Loss: 6.266696929931641\n",
      "Step: 11005/20000 | Training Loss: 5.94033670425415 | Validation Loss: 6.260474681854248\n",
      "Step: 11006/20000 | Training Loss: 6.553426265716553 | Validation Loss: 6.266486167907715\n",
      "Step: 11007/20000 | Training Loss: 5.949286460876465 | Validation Loss: 6.261340141296387\n",
      "Step: 11008/20000 | Training Loss: 5.843494892120361 | Validation Loss: 6.259847164154053\n",
      "Step: 11009/20000 | Training Loss: 6.101619720458984 | Validation Loss: 6.257940292358398\n",
      "Step: 11010/20000 | Training Loss: 5.61368989944458 | Validation Loss: 6.254015922546387\n",
      "Step: 11011/20000 | Training Loss: 5.3012213706970215 | Validation Loss: 6.258082866668701\n",
      "Step: 11012/20000 | Training Loss: 6.374223709106445 | Validation Loss: 6.252806663513184\n",
      "Step: 11013/20000 | Training Loss: 5.3333611488342285 | Validation Loss: 6.264673709869385\n",
      "Step: 11014/20000 | Training Loss: 5.367276191711426 | Validation Loss: 6.26603364944458\n",
      "Step: 11015/20000 | Training Loss: 5.632938861846924 | Validation Loss: 6.272398471832275\n",
      "Step: 11016/20000 | Training Loss: 5.88419771194458 | Validation Loss: 6.267495155334473\n",
      "Step: 11017/20000 | Training Loss: 5.812327861785889 | Validation Loss: 6.256287574768066\n",
      "Step: 11018/20000 | Training Loss: 5.948451995849609 | Validation Loss: 6.251670837402344\n",
      "Step: 11019/20000 | Training Loss: 5.827749729156494 | Validation Loss: 6.25227165222168\n",
      "Step: 11020/20000 | Training Loss: 5.523719787597656 | Validation Loss: 6.263082981109619\n",
      "Step: 11021/20000 | Training Loss: 5.861780166625977 | Validation Loss: 6.260753631591797\n",
      "Step: 11022/20000 | Training Loss: 6.415814399719238 | Validation Loss: 6.2587432861328125\n",
      "Step: 11023/20000 | Training Loss: 5.550529479980469 | Validation Loss: 6.268249988555908\n",
      "Step: 11024/20000 | Training Loss: 5.656676769256592 | Validation Loss: 6.275497913360596\n",
      "Step: 11025/20000 | Training Loss: 5.667177200317383 | Validation Loss: 6.275273323059082\n",
      "Step: 11026/20000 | Training Loss: 5.788319110870361 | Validation Loss: 6.274163246154785\n",
      "Step: 11027/20000 | Training Loss: 5.502736568450928 | Validation Loss: 6.278548717498779\n",
      "Step: 11028/20000 | Training Loss: 6.285852909088135 | Validation Loss: 6.297030448913574\n",
      "Step: 11029/20000 | Training Loss: 5.669408321380615 | Validation Loss: 6.273894786834717\n",
      "Step: 11030/20000 | Training Loss: 5.450102806091309 | Validation Loss: 6.2769551277160645\n",
      "Step: 11031/20000 | Training Loss: 5.706097602844238 | Validation Loss: 6.2737932205200195\n",
      "Step: 11032/20000 | Training Loss: 5.972162246704102 | Validation Loss: 6.260929107666016\n",
      "Step: 11033/20000 | Training Loss: 5.433999061584473 | Validation Loss: 6.273772239685059\n",
      "Step: 11034/20000 | Training Loss: 5.705105781555176 | Validation Loss: 6.275327682495117\n",
      "Step: 11035/20000 | Training Loss: 5.601675510406494 | Validation Loss: 6.267632961273193\n",
      "Step: 11036/20000 | Training Loss: 5.545053958892822 | Validation Loss: 6.274367332458496\n",
      "Step: 11037/20000 | Training Loss: 5.8579020500183105 | Validation Loss: 6.263457775115967\n",
      "Step: 11038/20000 | Training Loss: 6.246508598327637 | Validation Loss: 6.2623443603515625\n",
      "Step: 11039/20000 | Training Loss: 5.713202476501465 | Validation Loss: 6.26625919342041\n",
      "Step: 11040/20000 | Training Loss: 5.7533979415893555 | Validation Loss: 6.2679524421691895\n",
      "Step: 11041/20000 | Training Loss: 6.0162529945373535 | Validation Loss: 6.267245769500732\n",
      "Step: 11042/20000 | Training Loss: 5.674688816070557 | Validation Loss: 6.263278961181641\n",
      "Step: 11043/20000 | Training Loss: 6.227353096008301 | Validation Loss: 6.249927520751953\n",
      "Step: 11044/20000 | Training Loss: 6.0332841873168945 | Validation Loss: 6.287328720092773\n",
      "Step: 11045/20000 | Training Loss: 5.792378902435303 | Validation Loss: 6.2575273513793945\n",
      "Step: 11046/20000 | Training Loss: 5.177201747894287 | Validation Loss: 6.2635273933410645\n",
      "Step: 11047/20000 | Training Loss: 5.634419918060303 | Validation Loss: 6.263846397399902\n",
      "Step: 11048/20000 | Training Loss: 6.198978424072266 | Validation Loss: 6.254648208618164\n",
      "Step: 11049/20000 | Training Loss: 5.406443119049072 | Validation Loss: 6.259951114654541\n",
      "Step: 11050/20000 | Training Loss: 6.123193740844727 | Validation Loss: 6.250862121582031\n",
      "Step: 11051/20000 | Training Loss: 5.755857467651367 | Validation Loss: 6.256678581237793\n",
      "Step: 11052/20000 | Training Loss: 5.9291839599609375 | Validation Loss: 6.249597549438477\n",
      "Step: 11053/20000 | Training Loss: 5.763768196105957 | Validation Loss: 6.25870418548584\n",
      "Step: 11054/20000 | Training Loss: 5.633488178253174 | Validation Loss: 6.2716965675354\n",
      "Step: 11055/20000 | Training Loss: 5.673614025115967 | Validation Loss: 6.266097545623779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 11056/20000 | Training Loss: 6.448904991149902 | Validation Loss: 6.259054183959961\n",
      "Step: 11057/20000 | Training Loss: 5.133313179016113 | Validation Loss: 6.278114318847656\n",
      "Step: 11058/20000 | Training Loss: 5.514863967895508 | Validation Loss: 6.266664028167725\n",
      "Step: 11059/20000 | Training Loss: 6.3541741371154785 | Validation Loss: 6.2562713623046875\n",
      "Step: 11060/20000 | Training Loss: 5.469985008239746 | Validation Loss: 6.275471210479736\n",
      "Step: 11061/20000 | Training Loss: 6.201667308807373 | Validation Loss: 6.258304119110107\n",
      "Step: 11062/20000 | Training Loss: 6.136003017425537 | Validation Loss: 6.260190010070801\n",
      "Step: 11063/20000 | Training Loss: 6.498969078063965 | Validation Loss: 6.307629585266113\n",
      "Step: 11064/20000 | Training Loss: 6.26291036605835 | Validation Loss: 6.263864517211914\n",
      "Step: 11065/20000 | Training Loss: 5.679205894470215 | Validation Loss: 6.269524097442627\n",
      "Step: 11066/20000 | Training Loss: 4.995804309844971 | Validation Loss: 6.293070316314697\n",
      "Step: 11067/20000 | Training Loss: 6.034173965454102 | Validation Loss: 6.277389049530029\n",
      "Step: 11068/20000 | Training Loss: 5.70323371887207 | Validation Loss: 6.266964912414551\n",
      "Step: 11069/20000 | Training Loss: 5.367675304412842 | Validation Loss: 6.272591590881348\n",
      "Step: 11070/20000 | Training Loss: 5.627493381500244 | Validation Loss: 6.2672438621521\n",
      "Step: 11071/20000 | Training Loss: 6.094270706176758 | Validation Loss: 6.276383399963379\n",
      "Step: 11072/20000 | Training Loss: 5.806150436401367 | Validation Loss: 6.268653869628906\n",
      "Step: 11073/20000 | Training Loss: 5.281205654144287 | Validation Loss: 6.273809432983398\n",
      "Step: 11074/20000 | Training Loss: 5.760901927947998 | Validation Loss: 6.353532314300537\n",
      "Step: 11075/20000 | Training Loss: 4.74597692489624 | Validation Loss: 6.342563629150391\n",
      "Step: 11076/20000 | Training Loss: 6.202998161315918 | Validation Loss: 6.288346290588379\n",
      "Step: 11077/20000 | Training Loss: 5.724605083465576 | Validation Loss: 6.496502876281738\n",
      "Step: 11078/20000 | Training Loss: 6.242508411407471 | Validation Loss: 6.406020641326904\n",
      "Step: 11079/20000 | Training Loss: 6.215158462524414 | Validation Loss: 6.347781658172607\n",
      "Step: 11080/20000 | Training Loss: 5.587236404418945 | Validation Loss: 6.31213903427124\n",
      "Step: 11081/20000 | Training Loss: 6.287049770355225 | Validation Loss: 6.316915035247803\n",
      "Step: 11082/20000 | Training Loss: 5.6486992835998535 | Validation Loss: 6.292977809906006\n",
      "Step: 11083/20000 | Training Loss: 6.011601448059082 | Validation Loss: 6.279337406158447\n",
      "Step: 11084/20000 | Training Loss: 6.1144232749938965 | Validation Loss: 6.271282196044922\n",
      "Step: 11085/20000 | Training Loss: 5.879359245300293 | Validation Loss: 6.264463424682617\n",
      "Step: 11086/20000 | Training Loss: 5.440011024475098 | Validation Loss: 6.267884254455566\n",
      "Step: 11087/20000 | Training Loss: 6.0561676025390625 | Validation Loss: 6.270544052124023\n",
      "Step: 11088/20000 | Training Loss: 5.751418113708496 | Validation Loss: 6.2691121101379395\n",
      "Step: 11089/20000 | Training Loss: 5.7263503074646 | Validation Loss: 6.269309997558594\n",
      "Step: 11090/20000 | Training Loss: 5.519722938537598 | Validation Loss: 6.2728447914123535\n",
      "Step: 11091/20000 | Training Loss: 5.780646800994873 | Validation Loss: 6.265308856964111\n",
      "Step: 11092/20000 | Training Loss: 5.214731216430664 | Validation Loss: 6.2583394050598145\n",
      "Step: 11093/20000 | Training Loss: 6.197808265686035 | Validation Loss: 6.253607273101807\n",
      "Step: 11094/20000 | Training Loss: 5.731630802154541 | Validation Loss: 6.2623186111450195\n",
      "Step: 11095/20000 | Training Loss: 5.69442081451416 | Validation Loss: 6.261646747589111\n",
      "Step: 11096/20000 | Training Loss: 6.456453323364258 | Validation Loss: 6.264098644256592\n",
      "Step: 11097/20000 | Training Loss: 6.074162006378174 | Validation Loss: 6.261782646179199\n",
      "Step: 11098/20000 | Training Loss: 6.344743251800537 | Validation Loss: 6.268738746643066\n",
      "Step: 11099/20000 | Training Loss: 5.7057271003723145 | Validation Loss: 6.2652082443237305\n",
      "Step: 11100/20000 | Training Loss: 6.071907043457031 | Validation Loss: 6.274959087371826\n",
      "Step: 11101/20000 | Training Loss: 6.390681266784668 | Validation Loss: 6.272153854370117\n",
      "Step: 11102/20000 | Training Loss: 6.096249580383301 | Validation Loss: 6.264702320098877\n",
      "Step: 11103/20000 | Training Loss: 6.059451103210449 | Validation Loss: 6.270305633544922\n",
      "Step: 11104/20000 | Training Loss: 5.751998424530029 | Validation Loss: 6.255812168121338\n",
      "Step: 11105/20000 | Training Loss: 5.630140781402588 | Validation Loss: 6.254249572753906\n",
      "Step: 11106/20000 | Training Loss: 6.209169387817383 | Validation Loss: 6.25246000289917\n",
      "Step: 11107/20000 | Training Loss: 5.950050354003906 | Validation Loss: 6.2582197189331055\n",
      "Step: 11108/20000 | Training Loss: 5.474920749664307 | Validation Loss: 6.262557029724121\n",
      "Step: 11109/20000 | Training Loss: 5.3865742683410645 | Validation Loss: 6.273386478424072\n",
      "Step: 11110/20000 | Training Loss: 5.359591960906982 | Validation Loss: 6.274177074432373\n",
      "Step: 11111/20000 | Training Loss: 5.966004371643066 | Validation Loss: 6.2812323570251465\n",
      "Step: 11112/20000 | Training Loss: 6.1704630851745605 | Validation Loss: 6.28163480758667\n",
      "Step: 11113/20000 | Training Loss: 6.3992486000061035 | Validation Loss: 6.263601303100586\n",
      "Step: 11114/20000 | Training Loss: 6.032993793487549 | Validation Loss: 6.255926132202148\n",
      "Step: 11115/20000 | Training Loss: 5.832744598388672 | Validation Loss: 6.257225036621094\n",
      "Step: 11116/20000 | Training Loss: 5.8183369636535645 | Validation Loss: 6.249026775360107\n",
      "Step: 11117/20000 | Training Loss: 6.154914379119873 | Validation Loss: 6.295064449310303\n",
      "Step: 11118/20000 | Training Loss: 5.4672932624816895 | Validation Loss: 6.258829593658447\n",
      "Step: 11119/20000 | Training Loss: 6.139975070953369 | Validation Loss: 6.247409343719482\n",
      "Step: 11120/20000 | Training Loss: 5.888753414154053 | Validation Loss: 6.2488555908203125\n",
      "Step: 11121/20000 | Training Loss: 5.538145542144775 | Validation Loss: 6.248445987701416\n",
      "Step: 11122/20000 | Training Loss: 5.780088901519775 | Validation Loss: 6.250680923461914\n",
      "Step: 11123/20000 | Training Loss: 5.711984157562256 | Validation Loss: 6.249687671661377\n",
      "Step: 11124/20000 | Training Loss: 6.137802600860596 | Validation Loss: 6.250739097595215\n",
      "Step: 11125/20000 | Training Loss: 5.973144054412842 | Validation Loss: 6.254885673522949\n",
      "Step: 11126/20000 | Training Loss: 5.40037727355957 | Validation Loss: 6.259831428527832\n",
      "Step: 11127/20000 | Training Loss: 5.754313945770264 | Validation Loss: 6.260980129241943\n",
      "Step: 11128/20000 | Training Loss: 5.827564239501953 | Validation Loss: 6.258761405944824\n",
      "Step: 11129/20000 | Training Loss: 5.808858871459961 | Validation Loss: 6.254363536834717\n",
      "Step: 11130/20000 | Training Loss: 5.763604164123535 | Validation Loss: 6.255936622619629\n",
      "Step: 11131/20000 | Training Loss: 6.132821083068848 | Validation Loss: 6.255575180053711\n",
      "Step: 11132/20000 | Training Loss: 6.416278839111328 | Validation Loss: 6.250195503234863\n",
      "Step: 11133/20000 | Training Loss: 6.139882564544678 | Validation Loss: 6.262996673583984\n",
      "Step: 11134/20000 | Training Loss: 5.872110843658447 | Validation Loss: 6.253124713897705\n",
      "Step: 11135/20000 | Training Loss: 5.535122394561768 | Validation Loss: 6.264960289001465\n",
      "Step: 11136/20000 | Training Loss: 6.044687747955322 | Validation Loss: 6.2579169273376465\n",
      "Step: 11137/20000 | Training Loss: 5.762368202209473 | Validation Loss: 6.256312847137451\n",
      "Step: 11138/20000 | Training Loss: 5.543715000152588 | Validation Loss: 6.26579475402832\n",
      "Step: 11139/20000 | Training Loss: 6.38382625579834 | Validation Loss: 6.25851583480835\n",
      "Step: 11140/20000 | Training Loss: 5.85750675201416 | Validation Loss: 6.251450061798096\n",
      "Step: 11141/20000 | Training Loss: 5.8815999031066895 | Validation Loss: 6.255035400390625\n",
      "Step: 11142/20000 | Training Loss: 6.014520645141602 | Validation Loss: 6.251178741455078\n",
      "Step: 11143/20000 | Training Loss: 5.899000644683838 | Validation Loss: 6.2623090744018555\n",
      "Step: 11144/20000 | Training Loss: 5.748039245605469 | Validation Loss: 6.2618207931518555\n",
      "Step: 11145/20000 | Training Loss: 6.368308067321777 | Validation Loss: 6.25089168548584\n",
      "Step: 11146/20000 | Training Loss: 5.365607738494873 | Validation Loss: 6.256244659423828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 11147/20000 | Training Loss: 6.450538158416748 | Validation Loss: 6.326948642730713\n",
      "Step: 11148/20000 | Training Loss: 6.111885070800781 | Validation Loss: 6.268406391143799\n",
      "Step: 11149/20000 | Training Loss: 5.998831272125244 | Validation Loss: 6.253241539001465\n",
      "Step: 11150/20000 | Training Loss: 6.045167922973633 | Validation Loss: 6.251236438751221\n",
      "Step: 11151/20000 | Training Loss: 5.767307758331299 | Validation Loss: 6.248231410980225\n",
      "Step: 11152/20000 | Training Loss: 5.965418815612793 | Validation Loss: 6.258783340454102\n",
      "Step: 11153/20000 | Training Loss: 6.048044681549072 | Validation Loss: 6.267022132873535\n",
      "Step: 11154/20000 | Training Loss: 5.366024971008301 | Validation Loss: 6.271442890167236\n",
      "Step: 11155/20000 | Training Loss: 6.57291316986084 | Validation Loss: 6.260072708129883\n",
      "Step: 11156/20000 | Training Loss: 5.856303691864014 | Validation Loss: 6.2587761878967285\n",
      "Step: 11157/20000 | Training Loss: 6.207441806793213 | Validation Loss: 6.258338928222656\n",
      "Step: 11158/20000 | Training Loss: 5.668577194213867 | Validation Loss: 6.258884429931641\n",
      "Step: 11159/20000 | Training Loss: 6.001289367675781 | Validation Loss: 6.258427143096924\n",
      "Step: 11160/20000 | Training Loss: 5.936478614807129 | Validation Loss: 6.254520416259766\n",
      "Step: 11161/20000 | Training Loss: 5.910793781280518 | Validation Loss: 6.254990100860596\n",
      "Step: 11162/20000 | Training Loss: 5.903772830963135 | Validation Loss: 6.256304740905762\n",
      "Step: 11163/20000 | Training Loss: 6.082693099975586 | Validation Loss: 6.256485939025879\n",
      "Step: 11164/20000 | Training Loss: 6.475339889526367 | Validation Loss: 6.262509822845459\n",
      "Step: 11165/20000 | Training Loss: 5.45420503616333 | Validation Loss: 6.273959159851074\n",
      "Step: 11166/20000 | Training Loss: 6.377206802368164 | Validation Loss: 6.265038967132568\n",
      "Step: 11167/20000 | Training Loss: 5.882917404174805 | Validation Loss: 6.26530647277832\n",
      "Step: 11168/20000 | Training Loss: 5.8619866371154785 | Validation Loss: 6.265174388885498\n",
      "Step: 11169/20000 | Training Loss: 6.227975368499756 | Validation Loss: 6.265191555023193\n",
      "Step: 11170/20000 | Training Loss: 5.959879398345947 | Validation Loss: 6.280488967895508\n",
      "Step: 11171/20000 | Training Loss: 5.342150688171387 | Validation Loss: 6.276928424835205\n",
      "Step: 11172/20000 | Training Loss: 5.53731107711792 | Validation Loss: 6.281487464904785\n",
      "Step: 11173/20000 | Training Loss: 4.7851033210754395 | Validation Loss: 6.3212761878967285\n",
      "Step: 11174/20000 | Training Loss: 6.31736421585083 | Validation Loss: 6.278931617736816\n",
      "Step: 11175/20000 | Training Loss: 5.86884880065918 | Validation Loss: 6.273169040679932\n",
      "Step: 11176/20000 | Training Loss: 6.160646915435791 | Validation Loss: 6.2825493812561035\n",
      "Step: 11177/20000 | Training Loss: 5.649822235107422 | Validation Loss: 6.273045539855957\n",
      "Step: 11178/20000 | Training Loss: 5.638845443725586 | Validation Loss: 6.267666816711426\n",
      "Step: 11179/20000 | Training Loss: 5.866701602935791 | Validation Loss: 6.265432834625244\n",
      "Step: 11180/20000 | Training Loss: 6.4401421546936035 | Validation Loss: 6.264396667480469\n",
      "Step: 11181/20000 | Training Loss: 5.790513038635254 | Validation Loss: 6.269692897796631\n",
      "Step: 11182/20000 | Training Loss: 5.881138801574707 | Validation Loss: 6.266293048858643\n",
      "Step: 11183/20000 | Training Loss: 5.845612525939941 | Validation Loss: 6.257793426513672\n",
      "Step: 11184/20000 | Training Loss: 5.630721569061279 | Validation Loss: 6.261768817901611\n",
      "Step: 11185/20000 | Training Loss: 5.721335411071777 | Validation Loss: 6.260768890380859\n",
      "Step: 11186/20000 | Training Loss: 5.30522346496582 | Validation Loss: 6.280091285705566\n",
      "Step: 11187/20000 | Training Loss: 6.023263454437256 | Validation Loss: 6.266280174255371\n",
      "Step: 11188/20000 | Training Loss: 5.67708683013916 | Validation Loss: 6.266233921051025\n",
      "Step: 11189/20000 | Training Loss: 6.148116111755371 | Validation Loss: 6.261288166046143\n",
      "Step: 11190/20000 | Training Loss: 5.925536155700684 | Validation Loss: 6.259105682373047\n",
      "Step: 11191/20000 | Training Loss: 5.2446136474609375 | Validation Loss: 6.267115116119385\n",
      "Step: 11192/20000 | Training Loss: 5.251760482788086 | Validation Loss: 6.2764177322387695\n",
      "Step: 11193/20000 | Training Loss: 5.458631992340088 | Validation Loss: 6.277256965637207\n",
      "Step: 11194/20000 | Training Loss: 5.8570170402526855 | Validation Loss: 6.264069080352783\n",
      "Step: 11195/20000 | Training Loss: 6.02003288269043 | Validation Loss: 6.266624927520752\n",
      "Step: 11196/20000 | Training Loss: 5.853458404541016 | Validation Loss: 6.274607181549072\n",
      "Step: 11197/20000 | Training Loss: 5.925060749053955 | Validation Loss: 6.2675065994262695\n",
      "Step: 11198/20000 | Training Loss: 5.853019714355469 | Validation Loss: 6.267533302307129\n",
      "Step: 11199/20000 | Training Loss: 5.855320453643799 | Validation Loss: 6.270132064819336\n",
      "Step: 11200/20000 | Training Loss: 5.8593220710754395 | Validation Loss: 6.262221336364746\n",
      "Step: 11201/20000 | Training Loss: 5.334800720214844 | Validation Loss: 6.278168201446533\n",
      "Step: 11202/20000 | Training Loss: 5.811091899871826 | Validation Loss: 6.268002986907959\n",
      "Step: 11203/20000 | Training Loss: 5.935712814331055 | Validation Loss: 6.256915092468262\n",
      "Step: 11204/20000 | Training Loss: 5.798190593719482 | Validation Loss: 6.261133670806885\n",
      "Step: 11205/20000 | Training Loss: 6.036242961883545 | Validation Loss: 6.261274337768555\n",
      "Step: 11206/20000 | Training Loss: 6.449378967285156 | Validation Loss: 6.258655548095703\n",
      "Step: 11207/20000 | Training Loss: 5.666909217834473 | Validation Loss: 6.256370544433594\n",
      "Step: 11208/20000 | Training Loss: 5.7887163162231445 | Validation Loss: 6.253788471221924\n",
      "Step: 11209/20000 | Training Loss: 5.614211082458496 | Validation Loss: 6.260869026184082\n",
      "Step: 11210/20000 | Training Loss: 5.962222576141357 | Validation Loss: 6.253071308135986\n",
      "Step: 11211/20000 | Training Loss: 5.832398414611816 | Validation Loss: 6.256232261657715\n",
      "Step: 11212/20000 | Training Loss: 5.534353256225586 | Validation Loss: 6.248658180236816\n",
      "Step: 11213/20000 | Training Loss: 6.458948612213135 | Validation Loss: 6.247550010681152\n",
      "Step: 11214/20000 | Training Loss: 5.9893107414245605 | Validation Loss: 6.273035526275635\n",
      "Step: 11215/20000 | Training Loss: 5.132129192352295 | Validation Loss: 6.277975082397461\n",
      "Step: 11216/20000 | Training Loss: 5.805623531341553 | Validation Loss: 6.264059066772461\n",
      "Step: 11217/20000 | Training Loss: 5.979265213012695 | Validation Loss: 6.261563301086426\n",
      "Step: 11218/20000 | Training Loss: 5.402131080627441 | Validation Loss: 6.272675514221191\n",
      "Step: 11219/20000 | Training Loss: 5.472771644592285 | Validation Loss: 6.268775463104248\n",
      "Step: 11220/20000 | Training Loss: 5.1962995529174805 | Validation Loss: 6.284215450286865\n",
      "Step: 11221/20000 | Training Loss: 6.176264762878418 | Validation Loss: 6.259748935699463\n",
      "Step: 11222/20000 | Training Loss: 5.914278507232666 | Validation Loss: 6.2559380531311035\n",
      "Step: 11223/20000 | Training Loss: 6.272980690002441 | Validation Loss: 6.260374069213867\n",
      "Step: 11224/20000 | Training Loss: 5.494136810302734 | Validation Loss: 6.2617106437683105\n",
      "Step: 11225/20000 | Training Loss: 6.502260208129883 | Validation Loss: 6.254758834838867\n",
      "Step: 11226/20000 | Training Loss: 6.024967670440674 | Validation Loss: 6.250250339508057\n",
      "Step: 11227/20000 | Training Loss: 6.112249851226807 | Validation Loss: 6.252956390380859\n",
      "Step: 11228/20000 | Training Loss: 6.089531421661377 | Validation Loss: 6.254033088684082\n",
      "Step: 11229/20000 | Training Loss: 5.5895915031433105 | Validation Loss: 6.245458126068115\n",
      "Step: 11230/20000 | Training Loss: 6.471779823303223 | Validation Loss: 6.264622211456299\n",
      "Step: 11231/20000 | Training Loss: 6.045986652374268 | Validation Loss: 6.244674205780029\n",
      "Step: 11232/20000 | Training Loss: 5.763159275054932 | Validation Loss: 6.247227668762207\n",
      "Step: 11233/20000 | Training Loss: 5.810192108154297 | Validation Loss: 6.244683265686035\n",
      "Step: 11234/20000 | Training Loss: 5.300476551055908 | Validation Loss: 6.2617692947387695\n",
      "Step: 11235/20000 | Training Loss: 6.070390224456787 | Validation Loss: 6.325719833374023\n",
      "Step: 11236/20000 | Training Loss: 5.703848361968994 | Validation Loss: 6.254951477050781\n",
      "Step: 11237/20000 | Training Loss: 5.738025188446045 | Validation Loss: 6.25538969039917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 11238/20000 | Training Loss: 5.937983989715576 | Validation Loss: 6.244182586669922\n",
      "Step: 11239/20000 | Training Loss: 6.17411470413208 | Validation Loss: 6.2434587478637695\n",
      "Step: 11240/20000 | Training Loss: 5.50005578994751 | Validation Loss: 6.246013641357422\n",
      "Step: 11241/20000 | Training Loss: 5.651598930358887 | Validation Loss: 6.247166633605957\n",
      "Step: 11242/20000 | Training Loss: 5.468685150146484 | Validation Loss: 6.257784843444824\n",
      "Step: 11243/20000 | Training Loss: 5.774059772491455 | Validation Loss: 6.254687786102295\n",
      "Step: 11244/20000 | Training Loss: 5.666408538818359 | Validation Loss: 6.252106666564941\n",
      "Step: 11245/20000 | Training Loss: 5.74297571182251 | Validation Loss: 6.249843597412109\n",
      "Step: 11246/20000 | Training Loss: 6.385883808135986 | Validation Loss: 6.242934226989746\n",
      "Step: 11247/20000 | Training Loss: 6.058100700378418 | Validation Loss: 6.249375343322754\n",
      "Step: 11248/20000 | Training Loss: 5.42416524887085 | Validation Loss: 6.252216815948486\n",
      "Step: 11249/20000 | Training Loss: 5.208216667175293 | Validation Loss: 6.271055698394775\n",
      "Step: 11250/20000 | Training Loss: 5.5924835205078125 | Validation Loss: 6.254668712615967\n",
      "Step: 11251/20000 | Training Loss: 6.100131988525391 | Validation Loss: 6.249251842498779\n",
      "Step: 11252/20000 | Training Loss: 5.638635635375977 | Validation Loss: 6.254179954528809\n",
      "Step: 11253/20000 | Training Loss: 5.187868118286133 | Validation Loss: 6.266049385070801\n",
      "Step: 11254/20000 | Training Loss: 5.875645160675049 | Validation Loss: 6.2595953941345215\n",
      "Step: 11255/20000 | Training Loss: 6.1306915283203125 | Validation Loss: 6.2483673095703125\n",
      "Step: 11256/20000 | Training Loss: 5.661221027374268 | Validation Loss: 6.2536187171936035\n",
      "Step: 11257/20000 | Training Loss: 6.082550525665283 | Validation Loss: 6.244818687438965\n",
      "Step: 11258/20000 | Training Loss: 6.229209899902344 | Validation Loss: 6.277430534362793\n",
      "Step: 11259/20000 | Training Loss: 5.594885349273682 | Validation Loss: 6.255029201507568\n",
      "Step: 11260/20000 | Training Loss: 5.297394752502441 | Validation Loss: 6.269753456115723\n",
      "Step: 11261/20000 | Training Loss: 5.597210884094238 | Validation Loss: 6.258710861206055\n",
      "Step: 11262/20000 | Training Loss: 6.364931106567383 | Validation Loss: 6.252609729766846\n",
      "Step: 11263/20000 | Training Loss: 5.926138401031494 | Validation Loss: 6.25040340423584\n",
      "Step: 11264/20000 | Training Loss: 6.189375400543213 | Validation Loss: 6.259207725524902\n",
      "Step: 11265/20000 | Training Loss: 5.792901039123535 | Validation Loss: 6.254344463348389\n",
      "Step: 11266/20000 | Training Loss: 6.574978828430176 | Validation Loss: 6.260461330413818\n",
      "Step: 11267/20000 | Training Loss: 5.862534999847412 | Validation Loss: 6.255208492279053\n",
      "Step: 11268/20000 | Training Loss: 5.481990814208984 | Validation Loss: 6.264328479766846\n",
      "Step: 11269/20000 | Training Loss: 5.457880973815918 | Validation Loss: 6.278669357299805\n",
      "Step: 11270/20000 | Training Loss: 6.037886142730713 | Validation Loss: 6.264506816864014\n",
      "Step: 11271/20000 | Training Loss: 6.465538501739502 | Validation Loss: 6.266294479370117\n",
      "Step: 11272/20000 | Training Loss: 6.5818610191345215 | Validation Loss: 6.268352031707764\n",
      "Step: 11273/20000 | Training Loss: 6.042080879211426 | Validation Loss: 6.26583194732666\n",
      "Step: 11274/20000 | Training Loss: 5.555961608886719 | Validation Loss: 6.262994289398193\n",
      "Step: 11275/20000 | Training Loss: 5.931441783905029 | Validation Loss: 6.263463020324707\n",
      "Step: 11276/20000 | Training Loss: 5.856551647186279 | Validation Loss: 6.2560343742370605\n",
      "Step: 11277/20000 | Training Loss: 5.701038837432861 | Validation Loss: 6.261209964752197\n",
      "Step: 11278/20000 | Training Loss: 5.648545742034912 | Validation Loss: 6.260054588317871\n",
      "Step: 11279/20000 | Training Loss: 5.834143161773682 | Validation Loss: 6.261649131774902\n",
      "Step: 11280/20000 | Training Loss: 6.210358142852783 | Validation Loss: 6.267838954925537\n",
      "Step: 11281/20000 | Training Loss: 5.71803092956543 | Validation Loss: 6.261877059936523\n",
      "Step: 11282/20000 | Training Loss: 6.306036472320557 | Validation Loss: 6.263856887817383\n",
      "Step: 11283/20000 | Training Loss: 5.315255165100098 | Validation Loss: 6.263737678527832\n",
      "Step: 11284/20000 | Training Loss: 5.665903568267822 | Validation Loss: 6.263412952423096\n",
      "Step: 11285/20000 | Training Loss: 6.481703758239746 | Validation Loss: 6.268259048461914\n",
      "Step: 11286/20000 | Training Loss: 5.789180755615234 | Validation Loss: 6.2579264640808105\n",
      "Step: 11287/20000 | Training Loss: 5.9861555099487305 | Validation Loss: 6.2612833976745605\n",
      "Step: 11288/20000 | Training Loss: 5.657157897949219 | Validation Loss: 6.257430553436279\n",
      "Step: 11289/20000 | Training Loss: 6.01672887802124 | Validation Loss: 6.260416030883789\n",
      "Step: 11290/20000 | Training Loss: 5.467523574829102 | Validation Loss: 6.26106071472168\n",
      "Step: 11291/20000 | Training Loss: 5.644983768463135 | Validation Loss: 6.260340690612793\n",
      "Step: 11292/20000 | Training Loss: 5.528345108032227 | Validation Loss: 6.278635501861572\n",
      "Step: 11293/20000 | Training Loss: 6.474581241607666 | Validation Loss: 6.259513854980469\n",
      "Step: 11294/20000 | Training Loss: 5.5550642013549805 | Validation Loss: 6.259078025817871\n",
      "Step: 11295/20000 | Training Loss: 5.527606964111328 | Validation Loss: 6.262303829193115\n",
      "Step: 11296/20000 | Training Loss: 5.461005687713623 | Validation Loss: 6.269540786743164\n",
      "Step: 11297/20000 | Training Loss: 5.777848243713379 | Validation Loss: 6.25516414642334\n",
      "Step: 11298/20000 | Training Loss: 6.246293067932129 | Validation Loss: 6.253955364227295\n",
      "Step: 11299/20000 | Training Loss: 5.245208740234375 | Validation Loss: 6.263908863067627\n",
      "Step: 11300/20000 | Training Loss: 5.3061957359313965 | Validation Loss: 6.275452136993408\n",
      "Step: 11301/20000 | Training Loss: 5.2284698486328125 | Validation Loss: 6.275672912597656\n",
      "Step: 11302/20000 | Training Loss: 5.308538436889648 | Validation Loss: 6.28904914855957\n",
      "Step: 11303/20000 | Training Loss: 5.671964168548584 | Validation Loss: 6.286552429199219\n",
      "Step: 11304/20000 | Training Loss: 5.89351224899292 | Validation Loss: 6.257543087005615\n",
      "Step: 11305/20000 | Training Loss: 5.7405476570129395 | Validation Loss: 6.259815692901611\n",
      "Step: 11306/20000 | Training Loss: 6.088189125061035 | Validation Loss: 6.258958339691162\n",
      "Step: 11307/20000 | Training Loss: 5.562465190887451 | Validation Loss: 6.275180339813232\n",
      "Step: 11308/20000 | Training Loss: 6.493699550628662 | Validation Loss: 6.2606425285339355\n",
      "Step: 11309/20000 | Training Loss: 5.810906410217285 | Validation Loss: 6.26906681060791\n",
      "Step: 11310/20000 | Training Loss: 6.230700969696045 | Validation Loss: 6.260822296142578\n",
      "Step: 11311/20000 | Training Loss: 6.067677974700928 | Validation Loss: 6.273037910461426\n",
      "Step: 11312/20000 | Training Loss: 5.43434476852417 | Validation Loss: 6.261996746063232\n",
      "Step: 11313/20000 | Training Loss: 6.437567710876465 | Validation Loss: 6.2448015213012695\n",
      "Step: 11314/20000 | Training Loss: 5.43274450302124 | Validation Loss: 6.259954929351807\n",
      "Step: 11315/20000 | Training Loss: 6.581336975097656 | Validation Loss: 6.258229732513428\n",
      "Step: 11316/20000 | Training Loss: 6.1890058517456055 | Validation Loss: 6.241841793060303\n",
      "Step: 11317/20000 | Training Loss: 6.111123085021973 | Validation Loss: 6.247983932495117\n",
      "Step: 11318/20000 | Training Loss: 5.635921955108643 | Validation Loss: 6.243631839752197\n",
      "Step: 11319/20000 | Training Loss: 5.687164306640625 | Validation Loss: 6.2576751708984375\n",
      "Step: 11320/20000 | Training Loss: 6.185837745666504 | Validation Loss: 6.250192165374756\n",
      "Step: 11321/20000 | Training Loss: 6.067459583282471 | Validation Loss: 6.258552551269531\n",
      "Step: 11322/20000 | Training Loss: 5.9504170417785645 | Validation Loss: 6.255794525146484\n",
      "Step: 11323/20000 | Training Loss: 6.060100555419922 | Validation Loss: 6.258636951446533\n",
      "Step: 11324/20000 | Training Loss: 5.626662731170654 | Validation Loss: 6.261754989624023\n",
      "Step: 11325/20000 | Training Loss: 6.0079216957092285 | Validation Loss: 6.257969379425049\n",
      "Step: 11326/20000 | Training Loss: 6.250443458557129 | Validation Loss: 6.242367267608643\n",
      "Step: 11327/20000 | Training Loss: 6.75682258605957 | Validation Loss: 6.27828311920166\n",
      "Step: 11328/20000 | Training Loss: 5.61469030380249 | Validation Loss: 6.244543075561523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 11329/20000 | Training Loss: 5.919845104217529 | Validation Loss: 6.238550662994385\n",
      "Step: 11330/20000 | Training Loss: 5.90155029296875 | Validation Loss: 6.23322868347168\n",
      "Step: 11331/20000 | Training Loss: 6.186261177062988 | Validation Loss: 6.231151103973389\n",
      "%---Saving the model---%\n",
      "Step: 11332/20000 | Training Loss: 6.0237507820129395 | Validation Loss: 6.230689525604248\n",
      "%---Saving the model---%\n",
      "Step: 11333/20000 | Training Loss: 5.844259738922119 | Validation Loss: 6.237310409545898\n",
      "Step: 11334/20000 | Training Loss: 5.683506011962891 | Validation Loss: 6.23783540725708\n",
      "Step: 11335/20000 | Training Loss: 5.411160469055176 | Validation Loss: 6.254985332489014\n",
      "Step: 11336/20000 | Training Loss: 6.014742374420166 | Validation Loss: 6.32317590713501\n",
      "Step: 11337/20000 | Training Loss: 6.0147857666015625 | Validation Loss: 6.280334949493408\n",
      "Step: 11338/20000 | Training Loss: 5.778575420379639 | Validation Loss: 6.2415690422058105\n",
      "Step: 11339/20000 | Training Loss: 5.166016101837158 | Validation Loss: 6.26224946975708\n",
      "Step: 11340/20000 | Training Loss: 6.132974147796631 | Validation Loss: 6.246538162231445\n",
      "Step: 11341/20000 | Training Loss: 5.356158256530762 | Validation Loss: 6.2538628578186035\n",
      "Step: 11342/20000 | Training Loss: 6.306346893310547 | Validation Loss: 6.254957675933838\n",
      "Step: 11343/20000 | Training Loss: 5.866506099700928 | Validation Loss: 6.241973876953125\n",
      "Step: 11344/20000 | Training Loss: 5.762307167053223 | Validation Loss: 6.24088191986084\n",
      "Step: 11345/20000 | Training Loss: 5.8760085105896 | Validation Loss: 6.234429836273193\n",
      "Step: 11346/20000 | Training Loss: 5.390778064727783 | Validation Loss: 6.254977703094482\n",
      "Step: 11347/20000 | Training Loss: 6.221977710723877 | Validation Loss: 6.245929718017578\n",
      "Step: 11348/20000 | Training Loss: 5.841640472412109 | Validation Loss: 6.24199104309082\n",
      "Step: 11349/20000 | Training Loss: 5.765919208526611 | Validation Loss: 6.23520040512085\n",
      "Step: 11350/20000 | Training Loss: 5.684892177581787 | Validation Loss: 6.2371134757995605\n",
      "Step: 11351/20000 | Training Loss: 6.013225078582764 | Validation Loss: 6.242921829223633\n",
      "Step: 11352/20000 | Training Loss: 6.317834377288818 | Validation Loss: 6.2403564453125\n",
      "Step: 11353/20000 | Training Loss: 5.576735019683838 | Validation Loss: 6.237173557281494\n",
      "Step: 11354/20000 | Training Loss: 6.086921691894531 | Validation Loss: 6.24207067489624\n",
      "Step: 11355/20000 | Training Loss: 6.384675979614258 | Validation Loss: 6.252289772033691\n",
      "Step: 11356/20000 | Training Loss: 6.57099723815918 | Validation Loss: 6.237022876739502\n",
      "Step: 11357/20000 | Training Loss: 6.1015944480896 | Validation Loss: 6.252189636230469\n",
      "Step: 11358/20000 | Training Loss: 6.252355575561523 | Validation Loss: 6.262680530548096\n",
      "Step: 11359/20000 | Training Loss: 6.022874355316162 | Validation Loss: 6.250166416168213\n",
      "Step: 11360/20000 | Training Loss: 5.719123840332031 | Validation Loss: 6.2392258644104\n",
      "Step: 11361/20000 | Training Loss: 6.249845027923584 | Validation Loss: 6.256231784820557\n",
      "Step: 11362/20000 | Training Loss: 5.8445892333984375 | Validation Loss: 6.242007255554199\n",
      "Step: 11363/20000 | Training Loss: 6.057901859283447 | Validation Loss: 6.250626087188721\n",
      "Step: 11364/20000 | Training Loss: 5.783580303192139 | Validation Loss: 6.25176477432251\n",
      "Step: 11365/20000 | Training Loss: 6.303922653198242 | Validation Loss: 6.243122100830078\n",
      "Step: 11366/20000 | Training Loss: 5.65806770324707 | Validation Loss: 6.241700649261475\n",
      "Step: 11367/20000 | Training Loss: 6.284218788146973 | Validation Loss: 6.245309829711914\n",
      "Step: 11368/20000 | Training Loss: 6.020946979522705 | Validation Loss: 6.24000358581543\n",
      "Step: 11369/20000 | Training Loss: 6.093929290771484 | Validation Loss: 6.2860260009765625\n",
      "Step: 11370/20000 | Training Loss: 5.922553062438965 | Validation Loss: 6.2525315284729\n",
      "Step: 11371/20000 | Training Loss: 5.7559332847595215 | Validation Loss: 6.260525703430176\n",
      "Step: 11372/20000 | Training Loss: 5.819896221160889 | Validation Loss: 6.25754451751709\n",
      "Step: 11373/20000 | Training Loss: 6.170214653015137 | Validation Loss: 6.249719142913818\n",
      "Step: 11374/20000 | Training Loss: 5.6955084800720215 | Validation Loss: 6.256564617156982\n",
      "Step: 11375/20000 | Training Loss: 6.032388687133789 | Validation Loss: 6.26801061630249\n",
      "Step: 11376/20000 | Training Loss: 5.814537048339844 | Validation Loss: 6.262129783630371\n",
      "Step: 11377/20000 | Training Loss: 6.393261432647705 | Validation Loss: 6.251814365386963\n",
      "Step: 11378/20000 | Training Loss: 5.408923149108887 | Validation Loss: 6.2656073570251465\n",
      "Step: 11379/20000 | Training Loss: 5.443910121917725 | Validation Loss: 6.255221843719482\n",
      "Step: 11380/20000 | Training Loss: 5.914896488189697 | Validation Loss: 6.243569850921631\n",
      "Step: 11381/20000 | Training Loss: 5.368082523345947 | Validation Loss: 6.251354217529297\n",
      "Step: 11382/20000 | Training Loss: 5.523404598236084 | Validation Loss: 6.256201267242432\n"
     ]
    }
   ],
   "source": [
    "#tl=[]\n",
    "#vl=[]\n",
    "num_steps=20000\n",
    "val_loss_benchmark=vl[-1]\n",
    "for i in range(step,num_steps):\n",
    "    input_tensor_train,target_tensor_train=corpus.get_train_minibatch()\n",
    "    input_tensor_val,target_tensor_val=corpus.get_validation_batch()\n",
    "    input_tensor_train=Variable(input_tensor_train.cuda())\n",
    "    target_tensor_train=Variable(target_tensor_train.cuda())\n",
    "    input_tensor_val=Variable(input_tensor_val.cuda())\n",
    "    target_tensor_val=Variable(target_tensor_val.cuda())\n",
    "    train_loss=train(input_tensor_train,target_tensor_train , encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,corpus)\n",
    "    val_loss=validation(input_tensor_val,target_tensor_val, encoder, decoder, criterion,corpus)\n",
    "    tl.append(train_loss)\n",
    "    vl.append(val_loss)\n",
    "    print ('Step: {}/{} | Training Loss: {} | Validation Loss: {}'.format(i+1,num_steps,train_loss,val_loss))\n",
    "    \n",
    "    if (i>10 and val_loss<=val_loss_benchmark):\n",
    "            print ('%---Saving the model---%')\n",
    "            torch.save({\n",
    "                'step':i+1,\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "                'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "                'training_loss':tl,\n",
    "                'validation_loss':vl,\n",
    "                },'model.pth')\n",
    "            val_loss_benchmark=val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15160"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
