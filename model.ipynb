{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "from batchify import Corpus\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from model import Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=Corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda')\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,corpus):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    encoded, hidden=encoder(input_tensor)\n",
    "    batch_size=target_tensor.size(0)\n",
    "    decoder_input = np.zeros((batch_size,1))\n",
    "    decoder_input[:]=corpus.w2i['<sos>']\n",
    "    decoder_input=torch.from_numpy(decoder_input).cuda().detach()\n",
    "    dec_len=target_tensor.size(1)\n",
    "    decoded, hidden, outputs=decoder(encoded,hidden,decoder_input.long(),dec_len,target_tensor)\n",
    "    s=outputs.size(1)\n",
    "    loss=0\n",
    "    for i in range(s):\n",
    "        loss+=criterion(outputs[:,i,:],target_tensor[:,i])\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), 2)\n",
    "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), 2)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss/target_tensor.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(input_tensor, target_tensor, encoder, decoder, criterion,corpus):\n",
    "    with torch.no_grad():\n",
    "        encoded, hidden=encoder(input_tensor.long())\n",
    "        batch_size=target_tensor.size(0)\n",
    "        decoder_input = np.zeros((batch_size,1))\n",
    "        decoder_input[:]=corpus.w2i['<sos>']\n",
    "        decoder_input=torch.from_numpy(decoder_input).cuda().detach()\n",
    "        dec_len=target_tensor.size(1)\n",
    "        decoded, hidden,outputs=decoder(encoded,hidden,decoder_input.long(),dec_len,target_tensor,val=True)\n",
    "        s=outputs.size(1)\n",
    "        loss=0\n",
    "        for i in range(s):\n",
    "            loss+=criterion(outputs[:,i,:],target_tensor[:,i].long())\n",
    "        \n",
    "    return loss/target_tensor.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=Encoder(vocab_size=50000, embedding_size=128, hidden_size=256)\n",
    "decoder=Decoder(vocab_size=50000, embedding_dim=128, hidden_dim=256)\n",
    "encoder=encoder.to(device)\n",
    "decoder=decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder_optimizer = optim.Adagrad(encoder.parameters(), lr=0.15, initial_accumulator_value=0.1)\n",
    "#decoder_optimizer = optim.Adagrad(decoder.parameters(), lr=0.15, initial_accumulator_value=0.1)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters())\n",
    "decoder_optimizer = optim.Adam(decoder.parameters())\n",
    "criterion=nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (attn): Linear(in_features=500, out_features=400, bias=True)\n",
       "  (attn_combine): Linear(in_features=500, out_features=250, bias=True)\n",
       "  (linear1): Linear(in_features=500, out_features=250, bias=True)\n",
       "  (linear2): Linear(in_features=500, out_features=250, bias=True)\n",
       "  (embedding): Embedding(50000, 250)\n",
       "  (gru): GRU(250, 250, batch_first=True)\n",
       "  (linear): Linear(in_features=250, out_features=50000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checkpoint = torch.load('model.pth')\n",
    "#encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "#decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "#encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "#decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "#step = checkpoint['step']\n",
    "#tl = checkpoint['training_loss']\n",
    "#vl = checkpoint['validation_loss']\n",
    "\n",
    "#encoder.train()\n",
    "#decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.counter=47820\n",
    "#print(corpus.counter,step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1/10000 | Training Loss: 10.82170581817627 | Validation Loss: 10.730833053588867\n",
      "Step: 2/10000 | Training Loss: 10.699522972106934 | Validation Loss: 10.529882431030273\n",
      "Step: 3/10000 | Training Loss: 10.493638038635254 | Validation Loss: 10.283129692077637\n",
      "Step: 4/10000 | Training Loss: 10.087101936340332 | Validation Loss: 9.895716667175293\n",
      "Step: 5/10000 | Training Loss: 9.86425495147705 | Validation Loss: 9.364771842956543\n",
      "Step: 6/10000 | Training Loss: 9.708233833312988 | Validation Loss: 8.783259391784668\n",
      "Step: 7/10000 | Training Loss: 9.028059005737305 | Validation Loss: 8.28476333618164\n",
      "Step: 8/10000 | Training Loss: 8.010141372680664 | Validation Loss: 7.925635814666748\n",
      "Step: 9/10000 | Training Loss: 7.960357666015625 | Validation Loss: 7.657452583312988\n",
      "Step: 10/10000 | Training Loss: 7.234936237335205 | Validation Loss: 7.4276323318481445\n",
      "Step: 11/10000 | Training Loss: 7.411562442779541 | Validation Loss: 7.225571155548096\n",
      "Step: 12/10000 | Training Loss: 6.048364639282227 | Validation Loss: 7.048728942871094\n",
      "%---Saving the model---%\n",
      "Step: 13/10000 | Training Loss: 6.561412334442139 | Validation Loss: 6.896259784698486\n",
      "%---Saving the model---%\n",
      "Step: 14/10000 | Training Loss: 6.83335542678833 | Validation Loss: 6.746510028839111\n",
      "%---Saving the model---%\n",
      "Step: 15/10000 | Training Loss: 7.204282760620117 | Validation Loss: 6.625004291534424\n",
      "%---Saving the model---%\n",
      "Step: 16/10000 | Training Loss: 6.734373092651367 | Validation Loss: 6.5040764808654785\n",
      "%---Saving the model---%\n",
      "Step: 17/10000 | Training Loss: 6.317312240600586 | Validation Loss: 6.389627933502197\n",
      "%---Saving the model---%\n",
      "Step: 18/10000 | Training Loss: 6.420979976654053 | Validation Loss: 6.317877292633057\n",
      "%---Saving the model---%\n",
      "Step: 19/10000 | Training Loss: 6.55968713760376 | Validation Loss: 6.295848369598389\n",
      "%---Saving the model---%\n",
      "Step: 20/10000 | Training Loss: 5.918054103851318 | Validation Loss: 6.244271755218506\n",
      "%---Saving the model---%\n",
      "Step: 21/10000 | Training Loss: 6.175590515136719 | Validation Loss: 6.205502033233643\n",
      "%---Saving the model---%\n",
      "Step: 22/10000 | Training Loss: 6.0611572265625 | Validation Loss: 6.20628547668457\n",
      "Step: 23/10000 | Training Loss: 6.267836570739746 | Validation Loss: 6.165732383728027\n",
      "%---Saving the model---%\n",
      "Step: 24/10000 | Training Loss: 5.509443283081055 | Validation Loss: 6.1428141593933105\n",
      "%---Saving the model---%\n",
      "Step: 25/10000 | Training Loss: 5.790436744689941 | Validation Loss: 6.141221523284912\n",
      "%---Saving the model---%\n",
      "Step: 26/10000 | Training Loss: 6.0576701164245605 | Validation Loss: 6.106827259063721\n",
      "%---Saving the model---%\n",
      "Step: 27/10000 | Training Loss: 5.042999744415283 | Validation Loss: 6.100667953491211\n",
      "%---Saving the model---%\n",
      "Step: 28/10000 | Training Loss: 5.622772693634033 | Validation Loss: 6.14437198638916\n",
      "Step: 29/10000 | Training Loss: 5.360526084899902 | Validation Loss: 6.0549139976501465\n",
      "%---Saving the model---%\n",
      "Step: 30/10000 | Training Loss: 5.665964126586914 | Validation Loss: 6.051333427429199\n",
      "%---Saving the model---%\n",
      "Step: 31/10000 | Training Loss: 5.247673034667969 | Validation Loss: 6.177937030792236\n",
      "Step: 32/10000 | Training Loss: 5.6952805519104 | Validation Loss: 6.112733840942383\n",
      "Step: 33/10000 | Training Loss: 5.991174697875977 | Validation Loss: 6.084244251251221\n",
      "Step: 34/10000 | Training Loss: 6.118072032928467 | Validation Loss: 6.102838039398193\n",
      "Step: 35/10000 | Training Loss: 5.359825134277344 | Validation Loss: 6.045151710510254\n",
      "%---Saving the model---%\n",
      "Step: 36/10000 | Training Loss: 6.023186206817627 | Validation Loss: 6.070010185241699\n",
      "Step: 37/10000 | Training Loss: 6.421019554138184 | Validation Loss: 6.033475875854492\n",
      "%---Saving the model---%\n",
      "Step: 38/10000 | Training Loss: 5.9598612785339355 | Validation Loss: 6.014522552490234\n",
      "%---Saving the model---%\n",
      "Step: 39/10000 | Training Loss: 6.023488521575928 | Validation Loss: 6.029116630554199\n",
      "Step: 40/10000 | Training Loss: 5.470189571380615 | Validation Loss: 5.982439041137695\n",
      "%---Saving the model---%\n",
      "Step: 41/10000 | Training Loss: 6.074558734893799 | Validation Loss: 5.978067398071289\n",
      "%---Saving the model---%\n",
      "Step: 42/10000 | Training Loss: 5.991846561431885 | Validation Loss: 6.026941299438477\n",
      "Step: 43/10000 | Training Loss: 6.135879993438721 | Validation Loss: 6.039288520812988\n",
      "Step: 44/10000 | Training Loss: 5.549072742462158 | Validation Loss: 6.006173133850098\n",
      "Step: 45/10000 | Training Loss: 5.372586250305176 | Validation Loss: 6.162664413452148\n",
      "Step: 46/10000 | Training Loss: 5.919467926025391 | Validation Loss: 6.002148628234863\n",
      "Step: 47/10000 | Training Loss: 5.244359016418457 | Validation Loss: 6.124935626983643\n",
      "Step: 48/10000 | Training Loss: 5.515892028808594 | Validation Loss: 6.173280239105225\n"
     ]
    }
   ],
   "source": [
    "tl=[]\n",
    "vl=[]\n",
    "num_steps=10000\n",
    "val_loss_benchmark=10\n",
    "for i in range(0,num_steps):\n",
    "    input_tensor_train,target_tensor_train=corpus.get_train_minibatch()\n",
    "    input_tensor_val,target_tensor_val=corpus.get_validation_batch()\n",
    "    input_tensor_train=Variable(input_tensor_train.cuda())\n",
    "    target_tensor_train=Variable(target_tensor_train.cuda())\n",
    "    input_tensor_val=Variable(input_tensor_val.cuda())\n",
    "    target_tensor_val=Variable(target_tensor_val.cuda())\n",
    "    train_loss=train(input_tensor_train,target_tensor_train , encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,corpus)\n",
    "    val_loss=validation(input_tensor_val,target_tensor_val, encoder, decoder, criterion,corpus)\n",
    "    tl.append(train_loss)\n",
    "    vl.append(val_loss)\n",
    "    print ('Step: {}/{} | Training Loss: {} | Validation Loss: {}'.format(i+1,num_steps,train_loss,val_loss))\n",
    "    \n",
    "    if (i>10 and val_loss<=val_loss_benchmark):\n",
    "            print ('%---Saving the model---%')\n",
    "            torch.save({\n",
    "                'step':i+1,\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "                'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "                'training_loss':tl,\n",
    "                'validation_loss':vl,\n",
    "                'counter':corpus.counter\n",
    "                },'model.pth')\n",
    "            val_loss_benchmark=val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15160"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
